{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a205c-fd73-4567-9136-55bddd4e5dcc",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e94eaf-1f63-409c-8ecb-28fd5772b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import time  # Import the time module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b64d91-f02b-4780-94da-b1c832a55fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divi(arr, effectsize):\n",
    "    return [(1 / (num + 0.001)) * effectsize for num in arr]\n",
    "\n",
    "def multi(arr, effectsize):\n",
    "    return [num * effectsize for num in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b437e37a-a68c-4732-a495-296eb155702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae459b3-c1df-46d1-ab65-fd01386b9384",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/G5_L5_c5_k10_M0.1_HWE1/genotype/01_veryrare_genotype_AF_0.0_0.05.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m number_of_snps \u001b[38;5;241m=\u001b[39m (G\u001b[38;5;241m*\u001b[39mL)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# one loci per chromosome\u001b[39;00m\n\u001b[1;32m     45\u001b[0m number_of_individuals \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m*\u001b[39mk\u001b[38;5;241m*\u001b[39mk\n\u001b[0;32m---> 48\u001b[0m very_rare \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/G\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mG\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_L\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_c\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_k\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_M\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mM\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_HWE\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHWE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/genotype/01_veryrare_genotype_AF_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvery_rare_threshold_L\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvery_rare_threshold_H\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m rare \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_HWE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHWE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/01_rare_genotype_AF_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrare_threshold_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrare_threshold_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m common \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_HWE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHWE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/01_common_genotype_AF_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_threshold_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_threshold_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/abyss_simul/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/abyss_simul/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/G5_L5_c5_k10_M0.1_HWE1/genotype/01_veryrare_genotype_AF_0.0_0.05.pkl'"
     ]
    }
   ],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "    \n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "HWE = int(dict['HWE'])\n",
    "\n",
    "nr_humans = int(dict['nr_humans'])\n",
    "nr_snps = int(dict['nr_snps'])\n",
    "bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['PCA', 'abyss_counted', 'abyss', 'no_corr', 'abyss_pca']\n",
    "\n",
    "\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k\n",
    "\n",
    "\n",
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "\n",
    "if 'to_analyze' not in globals():\n",
    "    to_analyze = \"complete\"\n",
    "\n",
    "if to_analyze == \"complete\":\n",
    "    complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "elif to_analyze == \"common\":\n",
    "    complete = common.copy()\n",
    "    \n",
    "elif to_analyze == \"rare\":\n",
    "    complete = rare.copy()\n",
    "elif to_analyze == \"very_rare\":\n",
    "    complete = very_rare.copy()\n",
    "    \n",
    "complete = ((complete*2)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177b36e-9211-4628-b0fd-7579c724a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the environmental risks\n",
    "proto_naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"as_big_blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\",\n",
    "    \"gauss_blob_risk\" : \"Gaussian Risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"mid_mid_square_risk\": \"Mid square risk\",\n",
    "    \"hi_hyperbole_risk\": \"Hyperbole risk\",\n",
    "    \"sine_risk\": \"Sinusoidal risk\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefdcf4-e67f-466f-a6a1-f6a2cd048f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cc3e1-c454-4a23-bdb7-90377fb56b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'snp_effect' in scenarios:\n",
    "    naming_dict['no_risk'] = proto_naming_dict['no_risk']\n",
    "else:\n",
    "    pass\n",
    "if 'linear_continuous' in scenarios or 'mix_linear_continuous' in scenarios:\n",
    "    naming_dict['NW_risk'] = proto_naming_dict['NW_risk']\n",
    "    naming_dict['N_risk'] = proto_naming_dict['N_risk']\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'non_linear_continuous' in scenarios or 'mix_non_linear_continuous' in scenarios:\n",
    "    naming_dict['hi_gauss_blob_risk'] = proto_naming_dict['hi_gauss_blob_risk']\n",
    "    naming_dict['sine_risk'] = proto_naming_dict['sine_risk']\n",
    "    naming_dict['hi_hyperbole_risk'] = proto_naming_dict['hi_hyperbole_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_global' in scenarios or 'mix_discrete_global'in scenarios:\n",
    "    naming_dict['big_square_risk'] = proto_naming_dict['big_square_risk']\n",
    "    naming_dict['as_big_blob_risk'] = proto_naming_dict['as_big_blob_risk']\n",
    "    naming_dict['center_risk'] = proto_naming_dict['center_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_localized' in scenarios or 'mix_discrete_localized'in scenarios:\n",
    "    naming_dict['hi_square_risk'] = proto_naming_dict['hi_square_risk']\n",
    "    naming_dict['two_square_risk'] = proto_naming_dict['two_square_risk']\n",
    "    naming_dict['three_square_risk'] = proto_naming_dict['three_square_risk']\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1d8ee-c78e-4aa3-b58e-706408e85959",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = list(naming_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f37f2-2a83-4158-922c-b06772ba7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c68689-2f16-48bd-b0d8-3d011a21a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed693372-1c10-4437-9a48-7307c09e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8f0de-65a9-48d7-8d59-51f0faea61f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca752c9-bf34-41bb-8a20-b6cd60ed9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "if \"PCA\" in tools:\n",
    "    nr_complete_PCs = 8\n",
    "    pc_columns = ['PC{}'.format(i) for i in range(1, nr_complete_PCs+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80e0c9-0d03-4e63-8d22-492daeea1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'abyss' in tools or 'abyss_pca' in tools:\n",
    "    pq_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_2pqs\")][0]\n",
    "    p2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_p2s\")][0]\n",
    "    q2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_q2s\")][0]\n",
    "    \n",
    "    esti_p2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{p2_file}\")\n",
    "    esti_twopq = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{pq_file}\")\n",
    "    esti_q2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{q2_file}\")\n",
    "    \n",
    "    time_p2 = float(p2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_q2 = float(q2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_pq = float(pq_file.split('pop_')[1].split('seconds')[0])\n",
    "    total_abyss_time = np.round(time_p2+time_q2+time_pq,3)\n",
    "\n",
    "if 'abyss' in tools:\n",
    "    covariate_dictionary_esti_0 = {}\n",
    "    for snp in list(complete.columns):\n",
    "        temp = complete[[snp]].copy()\n",
    "        temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "        temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "        temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "        temp = temp.drop(columns=[snp])\n",
    "        covariate_dictionary_esti_0[snp] = temp\n",
    "\n",
    "if 'abyss_counted' in tools:\n",
    "    true_p2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_p2_via_true_pop.pkl\")\n",
    "    true_twopqs = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_twopq_via_true_pop.pkl\")\n",
    "    true_q2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_q2_via_true_pop.pkl\")\n",
    "    \n",
    "    covariate_dictionary_true = {}\n",
    "    for snp in list(complete.columns):\n",
    "        temp = complete[[snp]].copy()\n",
    "        temp[f\"{snp}_cov_p_q\"] = true_p2s[snp] - true_q2s[snp]\n",
    "        temp[f\"{snp}_cov_2pq\"] = true_twopqs[snp]\n",
    "        temp = temp.drop(columns=[snp])\n",
    "        covariate_dictionary_true[snp] = temp\n",
    "\n",
    "if 'abyss_pca' in tools:\n",
    "    nr_complete_PCs_abyss = 8\n",
    "    pc_columns_abyss = ['PC{}'.format(i) for i in range(1, nr_complete_PCs_abyss+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "    covariate_dictionary_combi = {}\n",
    "    for snp in list(complete.columns):\n",
    "        temp = complete[[snp]].copy()\n",
    "        temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "        temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "        temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "        temp[pc_columns_abyss] = PC_complete[pc_columns_abyss]\n",
    "        temp = temp.drop(columns=[snp])\n",
    "        covariate_dictionary_combi[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b80a38-8012-4816-a932-e495172d0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])\n",
    "noise = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf940b-b137-48b1-9b27-ef578ba7e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_effects = []\n",
    "if 'snp_effect' in scenarios or len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "    effectsize = 1\n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in common.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in common.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_common = pd.DataFrame(data)\n",
    "    beta_common['maf'] = \"common\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_rare = pd.DataFrame(data)\n",
    "    beta_rare['maf'] = \"rare\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in very_rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in very_rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_very_rare = pd.DataFrame(data)\n",
    "    beta_very_rare['maf'] = \"very rare\"\n",
    "    \n",
    "    betas = pd.concat([beta_common, beta_rare, beta_very_rare], ignore_index=True)\n",
    "\n",
    "    all_snps = list(complete.columns)\n",
    "    phenos_mono = []\n",
    "    for snp in all_snps:\n",
    "        index_snp = snp.split('_')[0]\n",
    "        beta_value = betas.loc[betas['snp'] == index_snp, 'Beta'].values[0]\n",
    "        phenos_mono.append(complete[snp] * beta_value)\n",
    "    \n",
    "    # Converting phenos_mono list of series to DataFrame directly\n",
    "    phenos_mono = pd.concat(phenos_mono, axis=1)\n",
    "    phenos_mono.columns = complete.columns\n",
    "    \n",
    "    # Add noise\n",
    "    n = len(phenos_mono)\n",
    "    for snp in list(phenos_mono.columns):\n",
    "        var_effect = np.var(phenos_mono[snp])\n",
    "        total_variance = var_effect / 0.001\n",
    "        var_noise = total_variance - var_effect\n",
    "        sd_noise = np.sqrt(var_noise)\n",
    "        # Generate phenotype with noise\n",
    "        phenos_mono[snp] = phenos_mono[snp] + np.random.normal(0, sd_noise, n)\n",
    "        std_dev = np.std(phenos_mono[snp])\n",
    "        mean = np.mean(phenos_mono[snp])\n",
    "        phenos_mono[snp] = (phenos_mono[snp] - mean) / std_dev\n",
    "\n",
    "    if len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "        ratio_effects.append(0.2)\n",
    "        ratio_effects.append(0.5)\n",
    "        ratio_effects.append(0.7)\n",
    "    else:\n",
    "        pass\n",
    "    if 'snp_effect' in scenarios:\n",
    "        ratio_effects.append(1)\n",
    "else:\n",
    "    ratio_effects.append(0)\n",
    "    phenos_mono = pd.DataFrame()\n",
    "    phenos_mono[complete.columns[0]] = np.zeros(len(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680571-71da-4d7b-b029-b5e906ddfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/plots/\"\n",
    "path_pvals = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/pvals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9378-cbec-4b26-81f2-a110cf1ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_plots, exist_ok=True)\n",
    "os.makedirs(path_pvals, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9399540-4c50-491b-9367-7865546e500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = [0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d554f-75ff-4ad7-8a44-60810f6f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_risks = risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134ad-afc3-41c9-85a7-a28e0c18caca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performances = pd.DataFrame()\n",
    "computing = pd.DataFrame()\n",
    "\n",
    "for ratio_effect in ratio_effects:\n",
    "    ratio_environment = 1-ratio_effect\n",
    "    for noise in noises:\n",
    "        for name_risk in iter_risks:\n",
    "            risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/environmental_risks/risk_{name_risk}.pkl\")\n",
    "            populations[name_risk] = risk[name_risk]\n",
    "            df_agg = populations.groupby(['x', 'y']).agg({name_risk: 'mean'}).reset_index()\n",
    "            grid_df = df_agg.pivot(index='y', columns='x', values=name_risk)\n",
    "            sns.heatmap(grid_df, cmap='rocket_r', linewidths=.5, square=True, cbar=False)\n",
    "            \n",
    "            # Add a title to the heatmap\n",
    "            plt.title(f\"{naming_dict[name_risk]}\", fontsize=16)\n",
    "            plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "            plt.savefig(f\"{path_plots}/envriskmap_{name_risk}.png\", dpi=100)\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "            y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk]), noise))\n",
    "            # Calculate the standard deviation and mean\n",
    "            std_dev = np.std(y)\n",
    "            mean = np.mean(y)\n",
    "            # Standardize\n",
    "            y = (y - mean) / std_dev\n",
    "            risk['pheno'] = y\n",
    "            y_snps = phenos_mono.copy()\n",
    "            for snp in list(phenos_mono.columns):\n",
    "                y_snps[snp] = phenos_mono[snp]*ratio_effect + y*ratio_environment\n",
    "            y = y_snps.copy()\n",
    "\n",
    "\n",
    "            df_pvals = pd.DataFrame()\n",
    "            df_time = pd.DataFrame()\n",
    "            df_bests = pd.DataFrame()\n",
    "            \n",
    "            # Best correction\n",
    "            start_time = time.time()\n",
    "            df_best_corr = manhattan_linear(complete, y, risk[[f\"{name_risk}\"]])\n",
    "            end_time = time.time()\n",
    "            elapsed_time_best_corr = np.round(end_time - start_time,3)\n",
    "            \n",
    "            df_pvals[['snp','coefs','AFs']] = df_best_corr[['snp','coefs','AFs']]\n",
    "            df_pvals[\"-logP_best_corr\"] = df_best_corr['-logPs']\n",
    "            df_time[\"best_corr\"] = [elapsed_time_best_corr]\n",
    "            df_bests[\"-logP_best_corr\"] = np.sort(df_best_corr['-logPs'])\n",
    "            \n",
    "            n = len(df_bests)\n",
    "            expected_quantiles = np.arange(1, n + 1) / n\n",
    "            expected_logP = np.sort(-np.log10(expected_quantiles))\n",
    "            df_bests['expected_logP'] = expected_logP\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_best_corr', data=df_bests, color='black', label=f\"Best correction\", linewidth=0)\n",
    "\n",
    "            if \"no_corr\" in tools:\n",
    "                # No correction\n",
    "                start_time = time.time()\n",
    "                df_no_corr = manhattan_linear(complete, y)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_no_corr = np.round(end_time - start_time,3)\n",
    "                df_pvals['-logPs'] = df_no_corr['-logPs']\n",
    "                df_time[\"no_corr\"] = [elapsed_time_no_corr]\n",
    "                df_bests[\"-logP_no_corr\"] = np.sort(df_no_corr['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_no_corr', data=df_bests, color='red', label='no covariate', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "\n",
    "\n",
    "            if \"x_y\" in tools:\n",
    "                # X and Y axis as covaraites\n",
    "                start_time = time.time()\n",
    "                pops = populations[['x','y']]\n",
    "                df_pops = manhattan_linear(complete, y , pops)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_x_y = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_x_y\"] = df_pops['-logPs']\n",
    "                df_time[\"x_y\"] = [elapsed_time_x_y]\n",
    "                df_bests[\"-logP_x_y\"] = np.sort(df_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_x_y', data=df_bests, color='darkblue', label='X,Y axis as covariate', linewidth=0)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss_counted\" in tools:\n",
    "                # p2 - q2 and 2pq as covariates\n",
    "                start_time = time.time()\n",
    "                df_p_q_2pq_covs_via_true_pops = manhattan_linear(complete, y, covariate_dictionary_true)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_true_ps = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_abyss_via_true_pops\"] = df_p_q_2pq_covs_via_true_pops['-logPs']\n",
    "                df_time[\"abyss_via_true_pops\"] = [elapsed_time_true_ps]\n",
    "                df_bests[\"-logP_abyss_via_true_pops\"] = np.sort(df_p_q_2pq_covs_via_true_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops', data=df_bests, color='orange', label='True MAFs as covs', linewidth=0)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if \"PCA\" in tools:\n",
    "                # PCs\n",
    "                start_time = time.time()\n",
    "                df_PCs = manhattan_linear(complete, y , PC_complete[pc_columns])\n",
    "                end_time = time.time()\n",
    "                elapsed_time_pcs = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_PCs\"] = df_PCs['-logPs']\n",
    "                df_time[\"PCs\"] = [elapsed_time_pcs]\n",
    "                df_bests[\"-logP_PCs\"] = np.sort(df_PCs['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_PCs', data=df_bests, color='pink', label=f\"{nr_complete_PCs} PCs as covariate\", linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            if \"abyss\" in tools:          \n",
    "                # Abyss\n",
    "                start_time = time.time()\n",
    "                df_abyss_p_q_2pq_covs_via_esti_pop = manhattan_linear(complete, y, covariate_dictionary_esti_0)            \n",
    "                end_time = time.time()\n",
    "                elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_abyss_pq2pq_via_esti_pops\"] = df_abyss_p_q_2pq_covs_via_esti_pop['-logPs']\n",
    "                df_time[\"abyss_pq2pq_via_esti_pops\"] = [elapsed_time_abyss]\n",
    "                df_bests[\"-logP_abyss_pq2pq_via_esti_pops\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops', data=df_bests, color='yellow', label='Estimates p2, q2, 2pq as covs', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            if \"abyss_pca\" in tools:\n",
    "                # Combines\n",
    "                start_time = time.time()\n",
    "                df_abyss_combined = manhattan_linear(complete, y, covariate_dictionary_combi)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_combined = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_combined\"] = df_abyss_combined['-logPs']\n",
    "                df_time[\"combined\"] = [elapsed_time_combined]\n",
    "                df_bests[\"-logP_combined\"] = np.sort(df_abyss_combined['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_combined', data=df_bests, color='green', label='combined', linewidth=0)\n",
    "\n",
    "            df_pvals.to_pickle(f\"{path_pvals}/P_vals_risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}.pkl\")\n",
    "            \n",
    "            # Find the maximum value in the DataFrame excluding inf and NaN\n",
    "            max_value = df_bests.replace([np.inf, -np.inf], np.nan).max().max()\n",
    "            \n",
    "            # Replace inf values with the maximum value found\n",
    "            df_bests.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "            \n",
    "            # Replace NaN values with the maximum value found\n",
    "            df_bests.fillna(max_value, inplace=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_differences = df_bests.subtract(df_bests['-logP_best_corr'], axis=0).abs()\n",
    "            performances[f\"risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}\"] = list(df_differences.mean(axis=0))\n",
    "            computing[f\"risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}\"] = list(df_time.T[0])\n",
    "        \n",
    "            \n",
    "            \n",
    "            # Plot diagonal reference line\n",
    "            plt.plot([min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     [min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     color='red', linestyle='--')\n",
    "            \n",
    "            # Set plot labels and title\n",
    "            plt.xlabel('Expected')\n",
    "            plt.ylabel('-Log10(P) Values')\n",
    "            plt.title(f\"QQ Plot of Log Values - {naming_dict[name_risk]}, causal/env = {ratio_effect}, phenotypic noise = {noise}\")\n",
    "            \n",
    "            # Show legend\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{path_plots}/qq_risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}.png\", dpi=100)\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    iter_risks = risks\n",
    "\n",
    "os.system(f\"rm -rf data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}\")\n",
    "performances['tools'] = list(df_bests.columns)\n",
    "computing['tools'] = list(df_time.columns)\n",
    "\n",
    "path_results = f\"data/results/\"\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "performances.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_performances.pkl\")\n",
    "computing.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_computingtimes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b25440-1b38-45d8-b536-809a622ee038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e3805-34e3-417c-b6cb-29f5504540c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
