{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a205c-fd73-4567-9136-55bddd4e5dcc",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e94eaf-1f63-409c-8ecb-28fd5772b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import time  # Import the time module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b64d91-f02b-4780-94da-b1c832a55fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divi(arr, effectsize):\n",
    "    return [(1 / (num + 0.001)) * effectsize for num in arr]\n",
    "\n",
    "def multi(arr, effectsize):\n",
    "    return [num * effectsize for num in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b437e37a-a68c-4732-a495-296eb155702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae459b3-c1df-46d1-ab65-fd01386b9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "    \n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "\n",
    "if 'snp_noise' not in globals():\n",
    "    snp_noise = 0.1\n",
    "HWE = int(dict['HWE'])\n",
    "\n",
    "nr_humans = int(dict['nr_humans'])\n",
    "nr_snps = int(dict['nr_snps'])\n",
    "bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['no_corr','abyss_counted', 'abyss', 'abyss_corrected', 'PCA','abyss_pca']\n",
    "\n",
    "\"\"\"\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\"\"\"\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['discrete_global']\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k\n",
    "\n",
    "\n",
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "\n",
    "if 'to_analyze' not in globals():\n",
    "    to_analyze = \"complete\"\n",
    "\n",
    "if to_analyze == \"complete\":\n",
    "    complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "elif to_analyze == \"common\":\n",
    "    complete = common.copy()\n",
    "    \n",
    "elif to_analyze == \"rare\":\n",
    "    complete = rare.copy()\n",
    "elif to_analyze == \"very_rare\":\n",
    "    complete = very_rare.copy()\n",
    "    \n",
    "complete = ((complete*2)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50faf7c0-ee00-4789-b2c0-c2f422bed4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no_corr', 'abyss_counted', 'abyss_corrected', 'PCA', 'abyss_pca']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b177b36e-9211-4628-b0fd-7579c724a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the environmental risks\n",
    "proto_naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"as_big_blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\",\n",
    "    \"gauss_blob_risk\" : \"Gaussian Risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"mid_mid_square_risk\": \"Mid square risk\",\n",
    "    \"hi_hyperbole_risk\": \"Hyperbole risk\",\n",
    "    \"sine_risk\": \"Sinusoidal risk\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecefdcf4-e67f-466f-a6a1-f6a2cd048f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66cc3e1-c454-4a23-bdb7-90377fb56b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'snp_effect' in scenarios:\n",
    "    naming_dict['no_risk'] = proto_naming_dict['no_risk']\n",
    "else:\n",
    "    pass\n",
    "if 'linear_continuous' in scenarios or 'mix_linear_continuous' in scenarios:\n",
    "    naming_dict['NW_risk'] = proto_naming_dict['NW_risk']\n",
    "    naming_dict['N_risk'] = proto_naming_dict['N_risk']\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'non_linear_continuous' in scenarios or 'mix_non_linear_continuous' in scenarios:\n",
    "    naming_dict['hi_gauss_blob_risk'] = proto_naming_dict['hi_gauss_blob_risk']\n",
    "    naming_dict['sine_risk'] = proto_naming_dict['sine_risk']\n",
    "    naming_dict['hi_hyperbole_risk'] = proto_naming_dict['hi_hyperbole_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_global' in scenarios or 'mix_discrete_global'in scenarios:\n",
    "    naming_dict['big_square_risk'] = proto_naming_dict['big_square_risk']\n",
    "    naming_dict['as_big_blob_risk'] = proto_naming_dict['as_big_blob_risk']\n",
    "    naming_dict['center_risk'] = proto_naming_dict['center_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_localized' in scenarios or 'mix_discrete_localized'in scenarios:\n",
    "    naming_dict['hi_square_risk'] = proto_naming_dict['hi_square_risk']\n",
    "    naming_dict['two_square_risk'] = proto_naming_dict['two_square_risk']\n",
    "    naming_dict['three_square_risk'] = proto_naming_dict['three_square_risk']\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a1d8ee-c78e-4aa3-b58e-706408e85959",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = list(naming_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31f37f2-2a83-4158-922c-b06772ba7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c68689-2f16-48bd-b0d8-3d011a21a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npopulations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\\npopulations[\\'population_number\\'] = populations[\\'populations\\'].str.extract(\\'(\\\\d+)\\').astype(int)\\n# Calculating X and Y coordinates\\npopulations[\\'x\\'] = ((populations[\\'population_number\\'] - 1) % k) + 1\\npopulations[\\'y\\'] = ((populations[\\'population_number\\'] - 1) // k) + 1\\npopulations[\\'z\\'] = 0.5\\npopulations[\\'population\\'] = populations[\\'population_number\\']/(k*k)\\npalette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations[\\'x\\'], populations[\\'y\\'], populations[\\'z\\'])]\\n\\n# Standardize the vector\\nstd_dev = np.std(populations[\\'x\\'])\\nmean = np.mean(populations[\\'x\\'])\\npopulations[\\'x\\'] = np.round((populations[\\'x\\'] - mean) / std_dev,2)\\n\\nstd_dev = np.std(populations[\\'y\\'])\\nmean = np.mean(populations[\\'y\\'])\\npopulations[\\'y\\'] = np.round((populations[\\'y\\'] - mean) / std_dev,2)\\n\\n# Check the grid\\ndf_agg = populations.groupby([\\'x\\', \\'y\\']).agg({\\'population\\': \\'mean\\'}).reset_index()\\n\\n# Now, pivot the aggregated DataFrame\\ngrid_df = df_agg.pivot(index=\\'y\\', columns=\\'x\\', values=\\'population\\')\\n\\n\\nheatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\\n\\n# Add a title to the heatmap\\nplt.title(\\'Population Grid\\', fontsize=16)\\nplt.gca().invert_yaxis()  # Sometimes it\\'s necessary to invert the y-axis for correct orientation\\n#plt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed693372-1c10-4437-9a48-7307c09e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8f0de-65a9-48d7-8d59-51f0faea61f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca752c9-bf34-41bb-8a20-b6cd60ed9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "if \"PCA\" in tools:\n",
    "    nr_complete_PCs = 8\n",
    "    pc_columns = ['PC{}'.format(i) for i in range(1, nr_complete_PCs+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c7b8723-1f68-424a-9a4f-aa8626dd18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja\n"
     ]
    }
   ],
   "source": [
    "if 'abyss' in tools or 'abyss_pca' in tools or 'abyss_corrected' in tools:\n",
    "    pq_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_2pqs\")][0]\n",
    "    p2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_p2s\")][0]\n",
    "    q2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_q2s\")][0]\n",
    "    \n",
    "    esti_p2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{p2_file}\")\n",
    "    esti_twopq = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{pq_file}\")\n",
    "    esti_q2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{q2_file}\")\n",
    "    \n",
    "    time_p2 = float(p2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_q2 = float(q2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_pq = float(pq_file.split('pop_')[1].split('seconds')[0])\n",
    "    total_abyss_time = np.round(time_p2+time_q2+time_pq,3)\n",
    "    \n",
    "    if 'abyss_corrected' in tools:\n",
    "        print('ja')\n",
    "        correction = pd.DataFrame()\n",
    "        for snp in list(complete.columns):\n",
    "            dummies = pd.get_dummies(complete[[snp]], columns=[snp], dtype=int)\n",
    "            dummies['corr'] = dummies[f\"{snp}_-1.0\"]*esti_q2[snp] + dummies[f\"{snp}_1.0\"]*esti_p2[snp]\n",
    "            correction[snp] = complete[snp]/dummies['corr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeedf2af-9c67-40c0-8086-02b9b40f3f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>snps</th>\n",
       "      <th>CG2_AF_0.3586</th>\n",
       "      <th>CG7_AF_0.3784</th>\n",
       "      <th>CG8_AF_0.2084</th>\n",
       "      <th>CG13_AF_0.2245</th>\n",
       "      <th>CG14_AF_0.2003</th>\n",
       "      <th>CG16_AF_0.2138</th>\n",
       "      <th>CG19_AF_0.4314</th>\n",
       "      <th>CG20_AF_0.2542</th>\n",
       "      <th>CG23_AF_0.4274</th>\n",
       "      <th>CG25_AF_0.2291</th>\n",
       "      <th>...</th>\n",
       "      <th>VRG136_AF_0.0457</th>\n",
       "      <th>VRG137_AF_0.0407</th>\n",
       "      <th>VRG147_AF_0.0414</th>\n",
       "      <th>VRG155_AF_0.0332</th>\n",
       "      <th>VRG160_AF_0.0478</th>\n",
       "      <th>VRG168_AF_0.0443</th>\n",
       "      <th>VRG170_AF_0.0469</th>\n",
       "      <th>VRG181_AF_0.0355</th>\n",
       "      <th>VRG194_AF_0.031</th>\n",
       "      <th>VRG199_AF_0.0496</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "snps  CG2_AF_0.3586  CG7_AF_0.3784  CG8_AF_0.2084  CG13_AF_0.2245  \\\n",
       "0               1.0            0.0            1.0             1.0   \n",
       "1              -1.0           -1.0            1.0             1.0   \n",
       "2              -1.0            0.0            1.0             1.0   \n",
       "3               0.0            1.0            1.0             1.0   \n",
       "4               1.0            0.0            1.0             1.0   \n",
       "...             ...            ...            ...             ...   \n",
       "4995            0.0            0.0            1.0             0.0   \n",
       "4996            1.0            1.0            1.0             0.0   \n",
       "4997           -1.0            1.0            1.0             0.0   \n",
       "4998            1.0           -1.0            1.0            -1.0   \n",
       "4999           -1.0            0.0            1.0             0.0   \n",
       "\n",
       "snps  CG14_AF_0.2003  CG16_AF_0.2138  CG19_AF_0.4314  CG20_AF_0.2542  \\\n",
       "0                1.0             0.0             1.0             1.0   \n",
       "1                1.0             0.0             1.0             1.0   \n",
       "2                1.0            -1.0             1.0             1.0   \n",
       "3                1.0            -1.0             1.0             0.0   \n",
       "4                1.0            -1.0             1.0             1.0   \n",
       "...              ...             ...             ...             ...   \n",
       "4995             1.0             1.0             1.0             1.0   \n",
       "4996             1.0             1.0             0.0             1.0   \n",
       "4997             1.0             1.0             1.0             1.0   \n",
       "4998             1.0             1.0             0.0             1.0   \n",
       "4999             1.0             1.0             0.0             1.0   \n",
       "\n",
       "snps  CG23_AF_0.4274  CG25_AF_0.2291  ...  VRG136_AF_0.0457  VRG137_AF_0.0407  \\\n",
       "0                0.0             1.0  ...               1.0               1.0   \n",
       "1                0.0             1.0  ...               1.0               1.0   \n",
       "2                0.0             1.0  ...               1.0               1.0   \n",
       "3                1.0             1.0  ...               1.0               1.0   \n",
       "4                0.0             1.0  ...               1.0               1.0   \n",
       "...              ...             ...  ...               ...               ...   \n",
       "4995             0.0             1.0  ...               1.0               1.0   \n",
       "4996             0.0             1.0  ...               1.0               1.0   \n",
       "4997             1.0             1.0  ...               1.0               1.0   \n",
       "4998             0.0             1.0  ...               1.0               1.0   \n",
       "4999            -1.0             1.0  ...               1.0               1.0   \n",
       "\n",
       "snps  VRG147_AF_0.0414  VRG155_AF_0.0332  VRG160_AF_0.0478  VRG168_AF_0.0443  \\\n",
       "0                  1.0               1.0               1.0               1.0   \n",
       "1                  1.0               1.0               1.0               1.0   \n",
       "2                  1.0               1.0               1.0               1.0   \n",
       "3                  1.0               1.0               1.0               1.0   \n",
       "4                  1.0               1.0               1.0               1.0   \n",
       "...                ...               ...               ...               ...   \n",
       "4995               0.0               1.0               1.0               1.0   \n",
       "4996               1.0               0.0               1.0               1.0   \n",
       "4997               1.0               1.0               1.0               1.0   \n",
       "4998               1.0               1.0               1.0               1.0   \n",
       "4999               1.0               1.0               1.0               1.0   \n",
       "\n",
       "snps  VRG170_AF_0.0469  VRG181_AF_0.0355  VRG194_AF_0.031  VRG199_AF_0.0496  \n",
       "0                  1.0               1.0              1.0               1.0  \n",
       "1                  1.0               1.0              1.0               1.0  \n",
       "2                  1.0               1.0              0.0               1.0  \n",
       "3                  1.0               1.0              1.0               1.0  \n",
       "4                  1.0               1.0              1.0               1.0  \n",
       "...                ...               ...              ...               ...  \n",
       "4995               1.0               1.0              1.0               1.0  \n",
       "4996               1.0               1.0              1.0               1.0  \n",
       "4997               1.0               1.0              1.0               1.0  \n",
       "4998               1.0               1.0              1.0               1.0  \n",
       "4999               0.0               1.0              1.0               1.0  \n",
       "\n",
       "[5000 rows x 191 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3089a536-eaa3-4f3e-9bea-aefb9c44f323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CG2_AF_0.3586</th>\n",
       "      <th>CG7_AF_0.3784</th>\n",
       "      <th>CG8_AF_0.2084</th>\n",
       "      <th>CG13_AF_0.2245</th>\n",
       "      <th>CG14_AF_0.2003</th>\n",
       "      <th>CG16_AF_0.2138</th>\n",
       "      <th>CG19_AF_0.4314</th>\n",
       "      <th>CG20_AF_0.2542</th>\n",
       "      <th>CG23_AF_0.4274</th>\n",
       "      <th>CG25_AF_0.2291</th>\n",
       "      <th>...</th>\n",
       "      <th>VRG136_AF_0.0457</th>\n",
       "      <th>VRG137_AF_0.0407</th>\n",
       "      <th>VRG147_AF_0.0414</th>\n",
       "      <th>VRG155_AF_0.0332</th>\n",
       "      <th>VRG160_AF_0.0478</th>\n",
       "      <th>VRG168_AF_0.0443</th>\n",
       "      <th>VRG170_AF_0.0469</th>\n",
       "      <th>VRG181_AF_0.0355</th>\n",
       "      <th>VRG194_AF_0.031</th>\n",
       "      <th>VRG199_AF_0.0496</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.772607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.626841</td>\n",
       "      <td>0.785663</td>\n",
       "      <td>0.754404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867934</td>\n",
       "      <td>0.763719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.736277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833389</td>\n",
       "      <td>0.928068</td>\n",
       "      <td>0.672196</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>0.732531</td>\n",
       "      <td>0.715245</td>\n",
       "      <td>0.707612</td>\n",
       "      <td>0.810643</td>\n",
       "      <td>0.695827</td>\n",
       "      <td>0.821542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.837293</td>\n",
       "      <td>-0.737059</td>\n",
       "      <td>0.768920</td>\n",
       "      <td>0.723052</td>\n",
       "      <td>0.671335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.929192</td>\n",
       "      <td>0.872508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.762014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782375</td>\n",
       "      <td>0.811384</td>\n",
       "      <td>0.634931</td>\n",
       "      <td>0.668541</td>\n",
       "      <td>0.816516</td>\n",
       "      <td>0.761681</td>\n",
       "      <td>0.615560</td>\n",
       "      <td>0.837674</td>\n",
       "      <td>0.721173</td>\n",
       "      <td>0.839147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.762185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703899</td>\n",
       "      <td>0.778301</td>\n",
       "      <td>0.713560</td>\n",
       "      <td>-0.937113</td>\n",
       "      <td>0.926221</td>\n",
       "      <td>0.925146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825795</td>\n",
       "      <td>0.824991</td>\n",
       "      <td>0.674693</td>\n",
       "      <td>0.679964</td>\n",
       "      <td>0.860370</td>\n",
       "      <td>0.888006</td>\n",
       "      <td>0.722923</td>\n",
       "      <td>0.807928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.713330</td>\n",
       "      <td>0.742669</td>\n",
       "      <td>0.763451</td>\n",
       "      <td>0.808160</td>\n",
       "      <td>-0.717759</td>\n",
       "      <td>0.827762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.919626</td>\n",
       "      <td>0.707665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693699</td>\n",
       "      <td>0.780307</td>\n",
       "      <td>0.664217</td>\n",
       "      <td>0.762397</td>\n",
       "      <td>0.830856</td>\n",
       "      <td>0.808020</td>\n",
       "      <td>0.672024</td>\n",
       "      <td>0.763484</td>\n",
       "      <td>0.777379</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.832315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.724954</td>\n",
       "      <td>0.710158</td>\n",
       "      <td>0.810455</td>\n",
       "      <td>-0.925197</td>\n",
       "      <td>0.784701</td>\n",
       "      <td>0.961218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748515</td>\n",
       "      <td>0.727073</td>\n",
       "      <td>0.673765</td>\n",
       "      <td>0.692422</td>\n",
       "      <td>0.787693</td>\n",
       "      <td>0.731924</td>\n",
       "      <td>0.751808</td>\n",
       "      <td>0.825232</td>\n",
       "      <td>0.683110</td>\n",
       "      <td>0.877377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823848</td>\n",
       "      <td>0.690143</td>\n",
       "      <td>0.976838</td>\n",
       "      <td>0.764139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.784745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846195</td>\n",
       "      <td>0.885863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.821486</td>\n",
       "      <td>0.665014</td>\n",
       "      <td>0.740323</td>\n",
       "      <td>0.753365</td>\n",
       "      <td>0.745041</td>\n",
       "      <td>0.705614</td>\n",
       "      <td>0.721329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.892799</td>\n",
       "      <td>0.976571</td>\n",
       "      <td>0.742362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815921</td>\n",
       "      <td>0.765377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.711627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.635780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736353</td>\n",
       "      <td>0.886281</td>\n",
       "      <td>0.765314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.684423</td>\n",
       "      <td>0.677112</td>\n",
       "      <td>0.737028</td>\n",
       "      <td>0.802415</td>\n",
       "      <td>0.837540</td>\n",
       "      <td>0.819273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-0.815839</td>\n",
       "      <td>0.715683</td>\n",
       "      <td>0.772340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.755214</td>\n",
       "      <td>0.716316</td>\n",
       "      <td>0.854116</td>\n",
       "      <td>0.800826</td>\n",
       "      <td>0.849585</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712736</td>\n",
       "      <td>0.947338</td>\n",
       "      <td>0.695594</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>0.692034</td>\n",
       "      <td>0.768243</td>\n",
       "      <td>0.864457</td>\n",
       "      <td>0.798958</td>\n",
       "      <td>0.739036</td>\n",
       "      <td>0.764850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1.005614</td>\n",
       "      <td>-0.689412</td>\n",
       "      <td>0.851767</td>\n",
       "      <td>-0.614255</td>\n",
       "      <td>0.741267</td>\n",
       "      <td>0.705982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.757875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769785</td>\n",
       "      <td>0.816315</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.782831</td>\n",
       "      <td>0.691576</td>\n",
       "      <td>0.828315</td>\n",
       "      <td>0.924045</td>\n",
       "      <td>0.750941</td>\n",
       "      <td>0.768146</td>\n",
       "      <td>0.815607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-0.791599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793006</td>\n",
       "      <td>0.705481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760829</td>\n",
       "      <td>-0.776719</td>\n",
       "      <td>0.798819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696726</td>\n",
       "      <td>0.833021</td>\n",
       "      <td>0.717598</td>\n",
       "      <td>0.747787</td>\n",
       "      <td>0.715695</td>\n",
       "      <td>0.762694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716158</td>\n",
       "      <td>0.814638</td>\n",
       "      <td>0.795187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CG2_AF_0.3586  CG7_AF_0.3784  CG8_AF_0.2084  CG13_AF_0.2245  \\\n",
       "0          0.772607            NaN       0.626841        0.785663   \n",
       "1         -0.837293      -0.737059       0.768920        0.723052   \n",
       "2         -0.762185            NaN       0.703899        0.778301   \n",
       "3               NaN       0.713330       0.742669        0.763451   \n",
       "4          0.832315            NaN       0.724954        0.710158   \n",
       "...             ...            ...            ...             ...   \n",
       "4995            NaN            NaN       0.995098             NaN   \n",
       "4996       0.892799       0.976571       0.742362             NaN   \n",
       "4997      -0.815839       0.715683       0.772340             NaN   \n",
       "4998       1.005614      -0.689412       0.851767       -0.614255   \n",
       "4999      -0.791599            NaN       0.851875             NaN   \n",
       "\n",
       "      CG14_AF_0.2003  CG16_AF_0.2138  CG19_AF_0.4314  CG20_AF_0.2542  \\\n",
       "0           0.754404             NaN        0.867934        0.763719   \n",
       "1           0.671335             NaN        0.929192        0.872508   \n",
       "2           0.713560       -0.937113        0.926221        0.925146   \n",
       "3           0.808160       -0.717759        0.827762             NaN   \n",
       "4           0.810455       -0.925197        0.784701        0.961218   \n",
       "...              ...             ...             ...             ...   \n",
       "4995        0.823848        0.690143        0.976838        0.764139   \n",
       "4996        0.815921        0.765377             NaN        0.711627   \n",
       "4997        0.755214        0.716316        0.854116        0.800826   \n",
       "4998        0.741267        0.705982             NaN        0.726704   \n",
       "4999        0.793006        0.705481             NaN        0.760829   \n",
       "\n",
       "      CG23_AF_0.4274  CG25_AF_0.2291  ...  VRG136_AF_0.0457  VRG137_AF_0.0407  \\\n",
       "0                NaN        0.736277  ...          0.833389          0.928068   \n",
       "1                NaN        0.762014  ...          0.782375          0.811384   \n",
       "2                NaN        0.705299  ...          0.825795          0.824991   \n",
       "3           0.919626        0.707665  ...          0.693699          0.780307   \n",
       "4                NaN        0.727926  ...          0.748515          0.727073   \n",
       "...              ...             ...  ...               ...               ...   \n",
       "4995             NaN        0.784745  ...          0.846195          0.885863   \n",
       "4996             NaN        0.635780  ...          0.736353          0.886281   \n",
       "4997        0.849585        0.872245  ...          0.712736          0.947338   \n",
       "4998             NaN        0.757875  ...          0.769785          0.816315   \n",
       "4999       -0.776719        0.798819  ...          0.696726          0.833021   \n",
       "\n",
       "      VRG147_AF_0.0414  VRG155_AF_0.0332  VRG160_AF_0.0478  VRG168_AF_0.0443  \\\n",
       "0             0.672196          0.676342          0.732531          0.715245   \n",
       "1             0.634931          0.668541          0.816516          0.761681   \n",
       "2             0.674693          0.679964          0.860370          0.888006   \n",
       "3             0.664217          0.762397          0.830856          0.808020   \n",
       "4             0.673765          0.692422          0.787693          0.731924   \n",
       "...                ...               ...               ...               ...   \n",
       "4995               NaN          0.821486          0.665014          0.740323   \n",
       "4996          0.765314               NaN          0.684423          0.677112   \n",
       "4997          0.695594          0.726200          0.692034          0.768243   \n",
       "4998          0.760100          0.782831          0.691576          0.828315   \n",
       "4999          0.717598          0.747787          0.715695          0.762694   \n",
       "\n",
       "      VRG170_AF_0.0469  VRG181_AF_0.0355  VRG194_AF_0.031  VRG199_AF_0.0496  \n",
       "0             0.707612          0.810643         0.695827          0.821542  \n",
       "1             0.615560          0.837674         0.721173          0.839147  \n",
       "2             0.722923          0.807928              NaN          0.733383  \n",
       "3             0.672024          0.763484         0.777379          0.822394  \n",
       "4             0.751808          0.825232         0.683110          0.877377  \n",
       "...                ...               ...              ...               ...  \n",
       "4995          0.753365          0.745041         0.705614          0.721329  \n",
       "4996          0.737028          0.802415         0.837540          0.819273  \n",
       "4997          0.864457          0.798958         0.739036          0.764850  \n",
       "4998          0.924045          0.750941         0.768146          0.815607  \n",
       "4999               NaN          0.716158         0.814638          0.795187  \n",
       "\n",
       "[5000 rows x 191 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80e0c9-0d03-4e63-8d22-492daeea1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'abyss' in tools or 'abyss_pca' in tools or 'abyss_corrected' in tools:\n",
    "    pq_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_2pqs\")][0]\n",
    "    p2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_p2s\")][0]\n",
    "    q2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_q2s\")][0]\n",
    "    \n",
    "    esti_p2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{p2_file}\")\n",
    "    esti_twopq = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{pq_file}\")\n",
    "    esti_q2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{q2_file}\")\n",
    "    \n",
    "    time_p2 = float(p2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_q2 = float(q2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_pq = float(pq_file.split('pop_')[1].split('seconds')[0])\n",
    "    total_abyss_time = np.round(time_p2+time_q2+time_pq,3)\n",
    "    if 'abyss_corrected' in tools:\n",
    "        correction = pd.DataFrame()\n",
    "        for snp in list(complete.columns):\n",
    "            dummies = pd.get_dummies(complete[[snp]], columns=[snp], dtype=int)\n",
    "\n",
    "            correction['snp'] = complete[snp]\n",
    "            correction['minaf'] = esti_q2[snp]\n",
    "            correction['hetaf'] = esti_twopq[snp]\n",
    "            correction['majaf'] = esti_p2[snp]\n",
    "            correction[snp] = correction.apply(lambda row: row['minaf'] if row['snp'] == -1 else (row['hetaf'] if row['snp'] == 0 else row['majaf']), axis=1)\n",
    "        correction = correction.drop(columns=['snp','minaf','hetaf','majaf'])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'abyss_counted' in tools:\n",
    "    true_p2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_p2_via_true_pop.pkl\")\n",
    "    true_twopqs = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_twopq_via_true_pop.pkl\")\n",
    "    true_q2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_q2_via_true_pop.pkl\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'abyss_pca' in tools:\n",
    "    nr_complete_PCs_abyss = 8\n",
    "    pc_columns_abyss = ['PC{}'.format(i) for i in range(1, nr_complete_PCs_abyss+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "    covariate_dictionary_combi = {}\n",
    "    for snp in list(complete.columns):\n",
    "        temp = complete[[snp]].copy()\n",
    "        temp[f\"{snp}_cov_p2minq2\"] = esti_p2[snp] - esti_q2[snp]\n",
    "        temp[pc_columns_abyss] = PC_complete[pc_columns_abyss]\n",
    "        temp = temp.drop(columns=[snp])\n",
    "        covariate_dictionary_combi[snp] = temp\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b80a38-8012-4816-a932-e495172d0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf940b-b137-48b1-9b27-ef578ba7e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_effects = []\n",
    "if 'snp_effect' in scenarios or len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "    effectsize = 1\n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in common.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in common.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_common = pd.DataFrame(data)\n",
    "    beta_common['maf'] = \"common\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_rare = pd.DataFrame(data)\n",
    "    beta_rare['maf'] = \"rare\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in very_rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in very_rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_very_rare = pd.DataFrame(data)\n",
    "    beta_very_rare['maf'] = \"very rare\"\n",
    "    \n",
    "    betas = pd.concat([beta_common, beta_rare, beta_very_rare], ignore_index=True)\n",
    "\n",
    "    all_snps = list(complete.columns)\n",
    "    phenos_mono = []\n",
    "    for snp in all_snps:\n",
    "        index_snp = snp.split('_')[0]\n",
    "        beta_value = betas.loc[betas['snp'] == index_snp, 'Beta'].values[0]\n",
    "        phenos_mono.append(complete[snp] * beta_value)\n",
    "    \n",
    "    # Converting phenos_mono list of series to DataFrame directly\n",
    "    phenos_mono = pd.concat(phenos_mono, axis=1)\n",
    "    phenos_mono.columns = complete.columns\n",
    "    \n",
    "    # Add noise\n",
    "    n = len(phenos_mono)\n",
    "    for snp in list(phenos_mono.columns):\n",
    "        var_effect = np.var(phenos_mono[snp])\n",
    "        total_variance = var_effect / snp_noise\n",
    "        var_noise = total_variance - var_effect\n",
    "        sd_noise = np.sqrt(var_noise)\n",
    "        # Generate phenotype with noise\n",
    "        phenos_mono[snp] = phenos_mono[snp] + np.random.normal(0, sd_noise, n)\n",
    "        std_dev = np.std(phenos_mono[snp])\n",
    "        mean = np.mean(phenos_mono[snp])\n",
    "        phenos_mono[snp] = (phenos_mono[snp] - mean) / std_dev\n",
    "\n",
    "    if len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "        ratio_effects.append(0.2)\n",
    "        ratio_effects.append(0.5)\n",
    "        ratio_effects.append(0.7)\n",
    "    else:\n",
    "        pass\n",
    "    if 'snp_effect' in scenarios:\n",
    "        ratio_effects.append(1)\n",
    "else:\n",
    "    ratio_effects.append(0)\n",
    "    phenos_mono = pd.DataFrame()\n",
    "    phenos_mono[complete.columns[0]] = np.zeros(len(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680571-71da-4d7b-b029-b5e906ddfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/plots/\"\n",
    "path_pvals = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/pvals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9378-cbec-4b26-81f2-a110cf1ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_plots, exist_ok=True)\n",
    "os.makedirs(path_pvals, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9399540-4c50-491b-9367-7865546e500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_noises = [0.1, 0.5]\n",
    "if 'snp_effect' in scenarios:\n",
    "    env_noises = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d554f-75ff-4ad7-8a44-60810f6f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_risks = risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f1d52-d51d-4934-b0e6-820467026666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134ad-afc3-41c9-85a7-a28e0c18caca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performances = pd.DataFrame()\n",
    "computing = pd.DataFrame()\n",
    "\n",
    "for ratio_effect in ratio_effects:\n",
    "    ratio_environment = 1-ratio_effect\n",
    "    for env_noise in env_noises:\n",
    "        for name_risk in iter_risks:\n",
    "            risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/environmental_risks/risk_{name_risk}.pkl\")\n",
    "            populations[name_risk] = risk[name_risk]\n",
    "            df_agg = populations.groupby(['x', 'y']).agg({name_risk: 'mean'}).reset_index()\n",
    "            grid_df = df_agg.pivot(index='y', columns='x', values=name_risk)\n",
    "            sns.heatmap(grid_df, cmap='rocket_r', linewidths=.5, square=True, cbar=False)\n",
    "            \n",
    "            # Add a title to the heatmap\n",
    "            plt.title(f\"{naming_dict[name_risk]}\", fontsize=16)\n",
    "            plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "            plt.savefig(f\"{path_plots}/envriskmap_{name_risk}.png\", dpi=100)\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "            y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk]), env_noise))\n",
    "            # Calculate the standard deviation and mean\n",
    "            std_dev = np.std(y)\n",
    "            mean = np.mean(y)\n",
    "            # Standardize\n",
    "            y = (y - mean) / std_dev\n",
    "            risk['pheno'] = y\n",
    "            y_snps = phenos_mono.copy()\n",
    "            for snp in list(phenos_mono.columns):\n",
    "                y_snps[snp] = phenos_mono[snp]*ratio_effect + y*ratio_environment\n",
    "            y = y_snps.copy()\n",
    "            \n",
    "            df_pvals = pd.DataFrame()\n",
    "            df_time = pd.DataFrame()\n",
    "            df_bests = pd.DataFrame()\n",
    "            \n",
    "            # Best correction\n",
    "            start_time = time.time()\n",
    "            df_best_corr = manhattan_linear(complete, y, risk[[f\"{name_risk}\"]])\n",
    "            end_time = time.time()\n",
    "            elapsed_time_best_corr = np.round(end_time - start_time,3)\n",
    "            \n",
    "            df_pvals[['snp','coefs','AFs']] = df_best_corr[['snp','coefs','AFs']]\n",
    "            df_pvals[\"-logP_best_corr\"] = df_best_corr['-logPs']\n",
    "            df_time[\"best_corr\"] = [elapsed_time_best_corr]\n",
    "            df_bests[\"-logP_best_corr\"] = np.sort(df_best_corr['-logPs'])\n",
    "            \n",
    "            n = len(df_bests)\n",
    "            expected_quantiles = np.arange(1, n + 1) / n\n",
    "            expected_logP = np.sort(-np.log10(expected_quantiles))\n",
    "            df_bests['expected_logP'] = expected_logP\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_best_corr', data=df_bests, color='black', label=f\"Best correction\", linewidth=0)\n",
    "\n",
    "            if \"no_corr\" in tools:\n",
    "                # No correction\n",
    "                start_time = time.time()\n",
    "                df_no_corr = manhattan_linear(complete, y)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_no_corr = np.round(end_time - start_time,3)\n",
    "                df_pvals['-logP_no_corr'] = df_no_corr['-logPs']\n",
    "                df_time[\"no_corr\"] = [elapsed_time_no_corr]\n",
    "                df_bests[\"-logP_no_corr\"] = np.sort(df_no_corr['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_no_corr', data=df_bests, color='red', label='no covariate', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "\n",
    "\n",
    "            if \"x_y\" in tools:\n",
    "                # X and Y axis as covaraites\n",
    "                start_time = time.time()\n",
    "                pops = populations[['x','y']]\n",
    "                df_pops = manhattan_linear(complete, y , pops)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_x_y = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_x_y\"] = df_pops['-logPs']\n",
    "                df_time[\"x_y\"] = [elapsed_time_x_y]\n",
    "                df_bests[\"-logP_x_y\"] = np.sort(df_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_x_y', data=df_bests, color='darkblue', label='X,Y axis as covariate', linewidth=0)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss_counted\" in tools:\n",
    "                # p2 - q2 and 2pq as covariates\n",
    "                start_time = time.time()\n",
    "                df_p_q_2pq_covs_via_true_pops = manhattan_linear(complete, y, true_p2s-true_q2s)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_true_ps = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_abyss_via_true_pops\"] = df_p_q_2pq_covs_via_true_pops['-logPs']\n",
    "                df_time[\"abyss_via_true_pops\"] = [elapsed_time_true_ps]\n",
    "                df_bests[\"-logP_abyss_via_true_pops\"] = np.sort(df_p_q_2pq_covs_via_true_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops', data=df_bests, color='orange', label='True MAFs as covs', linewidth=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"PCA\" in tools:\n",
    "                # PCs\n",
    "                start_time = time.time()\n",
    "                df_PCs = manhattan_linear(complete, y , PC_complete[pc_columns])\n",
    "                end_time = time.time()\n",
    "                elapsed_time_pcs = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_PCs\"] = df_PCs['-logPs']\n",
    "                df_time[\"PCs\"] = [elapsed_time_pcs]\n",
    "                df_bests[\"-logP_PCs\"] = np.sort(df_PCs['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_PCs', data=df_bests, color='pink', label=f\"{nr_complete_PCs} PCs as covariate\", linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss\" in tools:          \n",
    "                # Abyss\n",
    "                start_time = time.time()\n",
    "                df_abyss_p_q_2pq_covs_via_esti_pop = manhattan_linear(complete, y, esti_p2-esti_q2)            \n",
    "                end_time = time.time()\n",
    "                elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_abyss_pq2pq_via_esti_pops\"] = df_abyss_p_q_2pq_covs_via_esti_pop['-logPs']\n",
    "                df_time[\"abyss_pq2pq_via_esti_pops\"] = [elapsed_time_abyss]\n",
    "                df_bests[\"-logP_abyss_pq2pq_via_esti_pops\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops', data=df_bests, color='yellow', label='Estimates p2, q2, 2pq as covs', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if \"abyss_corrected\" in tools:          \n",
    "                # Abyss\n",
    "                X = complete.copy()\n",
    "                corr = esti_p2-esti_q2\n",
    "                start_time = time.time()\n",
    "                df_corrected = manhattan_linear(corr, y)\n",
    "                for snp in corr.columns:\n",
    "                    snp_index = snp.split('_AF_')[0]\n",
    "                    beta_value = df_corrected.loc[df_corrected['snp'] == snp_index, 'coefs'].values[0]\n",
    "                    corr[snp] = corr[snp]*beta_value\n",
    "                new_y = y - corr\n",
    "                df_abyss_p_q_2pq_covs_via_esti_pop_corrected = manhattan_linear(X-(esti_p2-esti_q2), new_y)            \n",
    "                end_time = time.time()\n",
    "                elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_abyss_pq2pq_via_esti_pops_corrected\"] = df_abyss_p_q_2pq_covs_via_esti_pop_corrected['-logPs']\n",
    "                df_time[\"abyss_pq2pq_via_esti_pops_corrected\"] = [elapsed_time_abyss]\n",
    "                df_bests[\"-logP_abyss_pq2pq_via_esti_pops_corrected\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop_corrected['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops_corrected', data=df_bests, color='blue', label='Estimates p2, q2, 2pq as covs + correction', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            if \"abyss_pca\" in tools:\n",
    "                # Combines\n",
    "                start_time = time.time()\n",
    "                df_abyss_combined = manhattan_linear(complete, y, covariate_dictionary_combi)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_combined = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_combined\"] = df_abyss_combined['-logPs']\n",
    "                df_time[\"combined\"] = [elapsed_time_combined]\n",
    "                df_bests[\"-logP_combined\"] = np.sort(df_abyss_combined['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_combined', data=df_bests, color='purple', label='Combined Abyss and PCA', linewidth=0)\n",
    "\n",
    "            df_pvals.to_pickle(f\"{path_pvals}/P_vals_risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}.pkl\")\n",
    "            \n",
    "            # Find the maximum value in the DataFrame excluding inf and NaN\n",
    "            max_value = df_bests.replace([np.inf, -np.inf], np.nan).max().max()\n",
    "            \n",
    "            # Replace inf values with the maximum value found\n",
    "            df_bests.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "            \n",
    "            # Replace NaN values with the maximum value found\n",
    "            df_bests.fillna(max_value, inplace=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_differences = df_bests.subtract(df_bests['-logP_best_corr'], axis=0).abs()\n",
    "            performances[f\"risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}\"] = list(df_differences.mean(axis=0))\n",
    "            computing[f\"risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}\"] = list(df_time.T[0])\n",
    "        \n",
    "            \n",
    "            \n",
    "            # Plot diagonal reference line\n",
    "            plt.plot([min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     [min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     color='red', linestyle='--')\n",
    "            \n",
    "            # Set plot labels and title\n",
    "            plt.xlabel('Expected')\n",
    "            plt.ylabel('-Log10(P) Values')\n",
    "            plt.title(f\"QQ Plot of Log Values - {naming_dict[name_risk]},\\nSNP effect={ratio_effect*100}%, noise={np.round(snp_noise*100)}%\\n Environment={np.round(ratio_environment*100)}%, noise={env_noise}%\")\n",
    "            \n",
    "            # Show legend\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{path_plots}/qq_risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}.png\", dpi=100)\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    iter_risks = risks\n",
    "\n",
    "os.system(f\"rm -rf data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}\")\n",
    "performances['tools'] = list(df_bests.columns)\n",
    "computing['tools'] = list(df_time.columns)\n",
    "\n",
    "path_results = f\"data/results/\"\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "performances.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_{snp_noise}_performances.pkl\")\n",
    "computing.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_{snp_noise}_computingtimes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb60fb-b406-4bce-8023-bf55b2b95893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
