{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f14c941-5e69-45a2-bdc7-46e09e1d1ded",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f771fa6-8444-40a1-91d0-ef572f9886e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import time  # Import the time module\n",
    "import importlib.util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t, entropy, stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Input, Model, layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "from helpers import (\n",
    "    parse_variables, get_risk_level, hi_gauss_blob_risk_fun, blob_risk_fun, \n",
    "    NW_risk_fun, square_risk_fun, map_to_color, simulate_quant_trait\n",
    ")\n",
    "\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11516e9c-9a16-4267-b872-359d52a16e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "if 'G' not in globals():\n",
    "    G = int(dict['G'])\n",
    "if 'L' not in globals():\n",
    "    L = int(dict['L'])\n",
    "if 'c' not in globals():\n",
    "    c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "if 'HWE' not in globals():\n",
    "    HWE = int(dict['HWE'])\n",
    "\n",
    "if 'bottleneck_nr' not in globals():\n",
    "    bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'nr_humans' not in globals():\n",
    "    nr_humans = int(dict['nr_humans'])\n",
    "\n",
    "if 'nr_snps' not in globals():\n",
    "    nr_snps = int(dict['nr_snps'])\n",
    "\n",
    "if 'epoch' not in globals():\n",
    "    epoch = 500\n",
    "if 'patience' not in globals():\n",
    "    patience = 100\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['PCA', 'abyss_counted', 'abyss', 'no_corr']\n",
    "\n",
    "\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = int((G*L)/2)\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9b5b34-cbfa-4446-9ce7-ce67aed3abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfcb4ec-50a3-4a2c-88a7-f457766e0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "complete = ((complete*2)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d625bb-e0ae-4636-a956-e0fa24abea4e",
   "metadata": {},
   "source": [
    "# Load populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e878ab77-22f4-42f9-9c58-b7f62131c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npopulations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\\npopulations[\\'population_number\\'] = populations[\\'populations\\'].str.extract(\\'(\\\\d+)\\').astype(int)\\n# Calculating X and Y coordinates\\npopulations[\\'x\\'] = ((populations[\\'population_number\\'] - 1) % k) + 1\\npopulations[\\'y\\'] = ((populations[\\'population_number\\'] - 1) // k) + 1\\npopulations[\\'z\\'] = 0.5\\npopulations[\\'population\\'] = populations[\\'population_number\\']/(k*k)\\npalette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations[\\'x\\'], populations[\\'y\\'], populations[\\'z\\'])]\\n\\n# Standardize the vector\\nstd_dev = np.std(populations[\\'x\\'])\\nmean = np.mean(populations[\\'x\\'])\\npopulations[\\'x\\'] = np.round((populations[\\'x\\'] - mean) / std_dev,2)\\n\\nstd_dev = np.std(populations[\\'y\\'])\\nmean = np.mean(populations[\\'y\\'])\\npopulations[\\'y\\'] = np.round((populations[\\'y\\'] - mean) / std_dev,2)\\n\\n# Check the grid\\ndf_agg = populations.groupby([\\'x\\', \\'y\\']).agg({\\'population\\': \\'mean\\'}).reset_index()\\n\\n# Now, pivot the aggregated DataFrame\\ngrid_df = df_agg.pivot(index=\\'y\\', columns=\\'x\\', values=\\'population\\')\\n\\n\\nheatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\\n\\n# Add a title to the heatmap\\nplt.title(\\'Population Grid\\', fontsize=16)\\nplt.gca().invert_yaxis()  # Sometimes it\\'s necessary to invert the y-axis for correct orientation\\n#plt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4690d-59f8-44a8-875d-a4cd3fa773cd",
   "metadata": {},
   "source": [
    "# Run abyss on everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9a7690-2584-4788-a61e-b22de3c528f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abyss(geno_in, geno_out, bottleneck_nr, epoch, patience):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(geno_in, geno_out, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Regularization parameter\n",
    "    l2_regularizer = 0.001\n",
    "    \n",
    "    # Original autoencoder model with L2 regularization\n",
    "    autoencoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100, activation='elu', input_shape=(geno_in.shape[1],), kernel_regularizer=regularizers.l2(l2_regularizer)),  # First hidden layer with L2 regularization\n",
    "        layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(bottleneck_nr, activation='elu', name='bottleneck', kernel_regularizer=regularizers.l2(l2_regularizer)),  # Bottleneck layer with L2 regularization\n",
    "        layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('elu'),\n",
    "        tf.keras.layers.Dense(100, activation='elu', kernel_regularizer=regularizers.l2(l2_regularizer)),  # Second hidden layer with L2 regularization\n",
    "        tf.keras.layers.Dense(geno_out.shape[1], activation='linear', kernel_regularizer=regularizers.l2(l2_regularizer))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Compile the original model with L2 regularization\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['mean_absolute_error'])\n",
    "    \n",
    "    # Define Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Fit the original model with Early Stopping\n",
    "    history = autoencoder.fit(X_train, y_train, epochs=epoch, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Extract the bottleneck layer after fitting the model\n",
    "    bottleneck_model = tf.keras.Model(inputs=autoencoder.inputs, outputs=autoencoder.get_layer('bottleneck').output)\n",
    "    \n",
    "    return autoencoder, bottleneck_model, history\n",
    "\n",
    "# Function to ensure minimum cluster size\n",
    "def ensure_min_cluster_size(data, labels, min_size):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    small_clusters = unique_labels[counts < min_size]\n",
    "    \n",
    "    # Reassign points from small clusters to the nearest large cluster\n",
    "    for cluster in small_clusters:\n",
    "        indices = np.where(labels == cluster)[0]\n",
    "        for index in indices:\n",
    "            # Find nearest large cluster\n",
    "            nearest_large_cluster = None\n",
    "            nearest_distance = float('inf')\n",
    "            for label in unique_labels:\n",
    "                if label not in small_clusters:\n",
    "                    distance = np.linalg.norm(data.iloc[index] - kmeans.cluster_centers_[label])\n",
    "                    if distance < nearest_distance:\n",
    "                        nearest_distance = distance\n",
    "                        nearest_large_cluster = label\n",
    "            labels[index] = nearest_large_cluster\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f1273-fdc2-438f-ad0a-8b0fcb992e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730281286.513432   45164 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "if 'abyss' in tools:\n",
    "    dim_labels = [f\"dim{i}\" for i in range(1, 2 + 1)]\n",
    "    autoencoder, bottleneck_model, history = abyss(complete, complete, 2, epoch, patience)\n",
    "    \n",
    "    path_bottle = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/abyss_bottleneck\"\n",
    "    os.system(f\"rm -rf {path_bottle}\")\n",
    "    os.makedirs(path_bottle, exist_ok=True)\n",
    "    \n",
    "    bottle_2dim = pd.DataFrame(data=bottleneck_model(complete), columns = dim_labels)\n",
    "    bottle_2dim.to_pickle(f\"{path_bottle}/abyss_bottleneck_2_dims_viz.pkl\")\n",
    "    \n",
    "    path_plots = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/plots/\"\n",
    "    os.makedirs(path_plots, exist_ok=True)\n",
    "\n",
    "\n",
    "    populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "    colors = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "    sns.scatterplot(x='dim1', y='dim2', data=bottle_2dim, color=colors, linewidth=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Autoencoder bottleneck - {number_of_snps} SNPs - {number_of_individuals} samples - Migration rate = {M}\")\n",
    "    # Show the plots\n",
    "    plt.savefig(f\"{path_plots}/bottleneck.png\", dpi=100)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    dim_labels = [f\"dim{i}\" for i in range(1, bottleneck_nr + 1)]\n",
    "    # start timer\n",
    "    start_time = time.time()\n",
    "    autoencoder, bottleneck_model, history = abyss(complete, complete, bottleneck_nr, epoch, patience)\n",
    "    # end timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = np.round(end_time - start_time,3)\n",
    "    print(f\"Time taken: {elapsed_time} seconds\")\n",
    "\n",
    "    bottle = pd.DataFrame(data=bottleneck_model(complete), columns = dim_labels)\n",
    "\n",
    "    sample_size = nr_humans\n",
    "    \n",
    "    num_clus = round(len(bottle) / sample_size)+1\n",
    "    \n",
    "    size_clus = int(len(bottle)/num_clus)\n",
    "    size_min = size_clus - round(size_clus / 5)\n",
    "    size_max = size_clus + round(size_clus / 5)\n",
    "    if num_clus <= 1:\n",
    "        bottle['cluster'] = 0\n",
    "    else:\n",
    "        # Apply constrained K-Means clustering\n",
    "        clf = KMeansConstrained(\n",
    "            n_clusters=num_clus,\n",
    "            size_min=size_min,\n",
    "            size_max=size_max,\n",
    "            random_state=0\n",
    "        )\n",
    "        clf.fit_predict(np.array(bottle))\n",
    "        bottle['cluster'] = clf.labels_\n",
    "\n",
    "    # If you want to shuffle instead of cluster\n",
    "    \"\"\"\n",
    "    # Assign cluster labels to genotypic data\n",
    "    count = Counter(clf.labels_)\n",
    "    empty = []\n",
    "    sample_size_temp = floor(len(clf.labels_)/sample_size)\n",
    "    for val in count.values():\n",
    "        multi = floor(val/sample_size_temp)+1\n",
    "        list_to_sample = multi*list(range(sample_size_temp))\n",
    "        random.shuffle(list_to_sample)\n",
    "        empty = empty + list_to_sample[0:val]\n",
    "    \n",
    "    bottle['cluster'] = empty\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    bottle_2dim['cluster'] = bottle['cluster']\n",
    "    bottle_2dim\n",
    "    cluster_counts = bottle_2dim['cluster'].value_counts()\n",
    "    \n",
    "    # Step 2: Update the 'cluster' column with the desired format\n",
    "    bottle_2dim['cluster'] = bottle_2dim['cluster'].apply(lambda x: f\"cluster {x} - {cluster_counts[x]} people\")\n",
    "\n",
    "    sns.scatterplot(x='dim1', y='dim2', data=bottle_2dim, hue='cluster', linewidth=0)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Autoencoder bottleneck - {number_of_snps} SNPs\\n{number_of_individuals} samples\\nMigration rate = {M}\")\n",
    "    # Show the plots\n",
    "    plt.savefig(f\"{path_plots}/clustering.png\", dpi=100)\n",
    "    #plt.show()\n",
    "    bottle.to_pickle(f\"{path_bottle}/abyss_bottleneck_{bottleneck_nr}_{elapsed_time}seconds.pkl\")\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
