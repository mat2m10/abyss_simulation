{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a205c-fd73-4567-9136-55bddd4e5dcc",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e94eaf-1f63-409c-8ecb-28fd5772b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import time  # Import the time module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b64d91-f02b-4780-94da-b1c832a55fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divi(arr, effectsize):\n",
    "    return [(1 / (num + 0.001)) * effectsize for num in arr]\n",
    "\n",
    "def multi(arr, effectsize):\n",
    "    return [num * effectsize for num in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b437e37a-a68c-4732-a495-296eb155702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae459b3-c1df-46d1-ab65-fd01386b9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "    \n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "\n",
    "if 'snp_noise' not in globals():\n",
    "    snp_noise = 0.1\n",
    "HWE = int(dict['HWE'])\n",
    "\n",
    "nr_humans = int(dict['nr_humans'])\n",
    "nr_snps = int(dict['nr_snps'])\n",
    "bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['no_corr','abyss_counted', 'abyss', 'PCA','abyss_pca']\n",
    "\n",
    "\"\"\"\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\"\"\"\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['discrete_global']\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k\n",
    "\n",
    "\n",
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "\n",
    "if 'to_analyze' not in globals():\n",
    "    to_analyze = \"complete\"\n",
    "\n",
    "if to_analyze == \"complete\":\n",
    "    complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "elif to_analyze == \"common\":\n",
    "    complete = common.copy()\n",
    "    \n",
    "elif to_analyze == \"rare\":\n",
    "    complete = rare.copy()\n",
    "elif to_analyze == \"very_rare\":\n",
    "    complete = very_rare.copy()\n",
    "    \n",
    "complete = ((complete*2)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b177b36e-9211-4628-b0fd-7579c724a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the environmental risks\n",
    "proto_naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"as_big_blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\",\n",
    "    \"gauss_blob_risk\" : \"Gaussian Risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"mid_mid_square_risk\": \"Mid square risk\",\n",
    "    \"hi_hyperbole_risk\": \"Hyperbole risk\",\n",
    "    \"sine_risk\": \"Sinusoidal risk\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecefdcf4-e67f-466f-a6a1-f6a2cd048f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66cc3e1-c454-4a23-bdb7-90377fb56b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'snp_effect' in scenarios:\n",
    "    naming_dict['no_risk'] = proto_naming_dict['no_risk']\n",
    "else:\n",
    "    pass\n",
    "if 'linear_continuous' in scenarios or 'mix_linear_continuous' in scenarios:\n",
    "    naming_dict['NW_risk'] = proto_naming_dict['NW_risk']\n",
    "    naming_dict['N_risk'] = proto_naming_dict['N_risk']\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'non_linear_continuous' in scenarios or 'mix_non_linear_continuous' in scenarios:\n",
    "    naming_dict['hi_gauss_blob_risk'] = proto_naming_dict['hi_gauss_blob_risk']\n",
    "    naming_dict['sine_risk'] = proto_naming_dict['sine_risk']\n",
    "    naming_dict['hi_hyperbole_risk'] = proto_naming_dict['hi_hyperbole_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_global' in scenarios or 'mix_discrete_global'in scenarios:\n",
    "    naming_dict['big_square_risk'] = proto_naming_dict['big_square_risk']\n",
    "    naming_dict['as_big_blob_risk'] = proto_naming_dict['as_big_blob_risk']\n",
    "    naming_dict['center_risk'] = proto_naming_dict['center_risk']\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "if 'discrete_localized' in scenarios or 'mix_discrete_localized'in scenarios:\n",
    "    naming_dict['hi_square_risk'] = proto_naming_dict['hi_square_risk']\n",
    "    naming_dict['two_square_risk'] = proto_naming_dict['two_square_risk']\n",
    "    naming_dict['three_square_risk'] = proto_naming_dict['three_square_risk']\n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a1d8ee-c78e-4aa3-b58e-706408e85959",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = list(naming_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31f37f2-2a83-4158-922c-b06772ba7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c68689-2f16-48bd-b0d8-3d011a21a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npopulations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\\npopulations[\\'population_number\\'] = populations[\\'populations\\'].str.extract(\\'(\\\\d+)\\').astype(int)\\n# Calculating X and Y coordinates\\npopulations[\\'x\\'] = ((populations[\\'population_number\\'] - 1) % k) + 1\\npopulations[\\'y\\'] = ((populations[\\'population_number\\'] - 1) // k) + 1\\npopulations[\\'z\\'] = 0.5\\npopulations[\\'population\\'] = populations[\\'population_number\\']/(k*k)\\npalette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations[\\'x\\'], populations[\\'y\\'], populations[\\'z\\'])]\\n\\n# Standardize the vector\\nstd_dev = np.std(populations[\\'x\\'])\\nmean = np.mean(populations[\\'x\\'])\\npopulations[\\'x\\'] = np.round((populations[\\'x\\'] - mean) / std_dev,2)\\n\\nstd_dev = np.std(populations[\\'y\\'])\\nmean = np.mean(populations[\\'y\\'])\\npopulations[\\'y\\'] = np.round((populations[\\'y\\'] - mean) / std_dev,2)\\n\\n# Check the grid\\ndf_agg = populations.groupby([\\'x\\', \\'y\\']).agg({\\'population\\': \\'mean\\'}).reset_index()\\n\\n# Now, pivot the aggregated DataFrame\\ngrid_df = df_agg.pivot(index=\\'y\\', columns=\\'x\\', values=\\'population\\')\\n\\n\\nheatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\\n\\n# Add a title to the heatmap\\nplt.title(\\'Population Grid\\', fontsize=16)\\nplt.gca().invert_yaxis()  # Sometimes it\\'s necessary to invert the y-axis for correct orientation\\n#plt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed693372-1c10-4437-9a48-7307c09e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8f0de-65a9-48d7-8d59-51f0faea61f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca752c9-bf34-41bb-8a20-b6cd60ed9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "if \"PCA\" in tools:\n",
    "    nr_complete_PCs = 8\n",
    "    pc_columns = ['PC{}'.format(i) for i in range(1, nr_complete_PCs+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80e0c9-0d03-4e63-8d22-492daeea1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'abyss' in tools or 'abyss_pca' in tools or 'abyss_corrected' in tools:\n",
    "    pq_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_2pqs\")][0]\n",
    "    p2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_p2s\")][0]\n",
    "    q2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_q2s\")][0]\n",
    "    \n",
    "    esti_p2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{p2_file}\")\n",
    "    esti_twopq = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{pq_file}\")\n",
    "    esti_q2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{q2_file}\")\n",
    "    \n",
    "    time_p2 = float(p2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_q2 = float(q2_file.split('pop_')[1].split('seconds')[0])\n",
    "    time_pq = float(pq_file.split('pop_')[1].split('seconds')[0])\n",
    "    total_abyss_time = np.round(time_p2+time_q2+time_pq,3)\n",
    "    if 'abyss_corrected' in tools:\n",
    "        correction = pd.DataFrame()\n",
    "        for snp in list(complete.columns):\n",
    "            correction['snp'] = complete[snp]\n",
    "            correction['minaf'] = esti_q2[snp]\n",
    "            correction['hetaf'] = esti_twopq[snp]\n",
    "            correction['majaf'] = esti_p2[snp]\n",
    "            correction[snp] = correction.apply(lambda row: row['minaf'] if row['snp'] == -1 else (row['hetaf'] if row['snp'] == 0 else row['majaf']), axis=1)\n",
    "        correction = correction.drop(columns=['snp','minaf','hetaf','majaf'])\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "#if 'abyss' in tools:\n",
    "    #covariate_dictionary_esti_0 = {}\n",
    "    #for snp in list(complete.columns):\n",
    "#        temp = complete[[snp]].copy()\n",
    "        #temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "        #temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "#        temp[f\"{snp}_cov_p2minq2\"] = esti_p2[snp] - esti_q2[snp]\n",
    "        \n",
    "        #temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "#        temp = temp.drop(columns=[snp])\n",
    "#        covariate_dictionary_esti_0[snp] = temp\n",
    "#else:\n",
    "#    pass\n",
    "    \n",
    "if 'abyss_counted' in tools or 'abyss_counted_corrected' in tools:\n",
    "    true_p2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_p2_via_true_pop.pkl\")\n",
    "    true_twopqs = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_twopq_via_true_pop.pkl\")\n",
    "    true_q2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_q2_via_true_pop.pkl\")\n",
    "    \n",
    "    #covariate_dictionary_true = {}\n",
    "    #for snp in list(complete.columns):\n",
    "    #    temp = complete[[snp]].copy()\n",
    "    #    temp[f\"{snp}_cov_p_q\"] = true_p2s[snp] - true_q2s[snp]\n",
    "    #    #temp[f\"{snp}_cov_2pq\"] = true_twopqs[snp]\n",
    "    #    temp = temp.drop(columns=[snp])\n",
    "    #    covariate_dictionary_true[snp] = temp\n",
    "\n",
    "    if 'abyss_counted_corrected' in tools:\n",
    "        correction_counted = pd.DataFrame()\n",
    "        for snp in list(complete.columns):\n",
    "            correction_counted['snp'] = complete[snp]\n",
    "            correction_counted['minaf'] = true_q2s[snp]\n",
    "            correction_counted['hetaf'] = true_twopqs[snp]\n",
    "            correction_counted['majaf'] = true_p2s[snp]\n",
    "            correction_counted[snp] = correction_counted.apply(lambda row: row['minaf'] if row['snp'] == -1 else (row['hetaf'] if row['snp'] == 0 else row['majaf']), axis=1)\n",
    "        correction_counted = correction_counted.drop(columns=['snp','minaf','hetaf','majaf'])\n",
    "else:\n",
    "    pass\n",
    "if 'abyss_pca' in tools:\n",
    "    nr_complete_PCs_abyss = 8\n",
    "    pc_columns_abyss = ['PC{}'.format(i) for i in range(1, nr_complete_PCs_abyss+1)]\n",
    "    PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")\n",
    "    covariate_dictionary_combi = {}\n",
    "    for snp in list(complete.columns):\n",
    "        temp = complete[[snp]].copy()\n",
    "        #temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "        #temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "        #temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "        temp[f\"{snp}_cov_p2minq2\"] = esti_p2[snp] - esti_q2[snp]\n",
    "        temp[pc_columns_abyss] = PC_complete[pc_columns_abyss]\n",
    "        temp = temp.drop(columns=[snp])\n",
    "        covariate_dictionary_combi[snp] = temp\n",
    "\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b80a38-8012-4816-a932-e495172d0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf940b-b137-48b1-9b27-ef578ba7e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_effects = []\n",
    "if 'snp_effect' in scenarios or len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "    effectsize = 1\n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in common.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in common.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_common = pd.DataFrame(data)\n",
    "    beta_common['maf'] = \"common\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_rare = pd.DataFrame(data)\n",
    "    beta_rare['maf'] = \"rare\"\n",
    "    \n",
    "    numbers_af = [float(col.split('_AF_')[1]) for col in very_rare.columns if '_AF_' in col]\n",
    "    snp_names = [col.split('_AF_')[0] for col in very_rare.columns if '_AF_' in col]\n",
    "    \n",
    "    beta_values = divi(numbers_af, effectsize)\n",
    "    data = {'snp': snp_names, 'Beta': beta_values}\n",
    "    beta_very_rare = pd.DataFrame(data)\n",
    "    beta_very_rare['maf'] = \"very rare\"\n",
    "    \n",
    "    betas = pd.concat([beta_common, beta_rare, beta_very_rare], ignore_index=True)\n",
    "\n",
    "    all_snps = list(complete.columns)\n",
    "    phenos_mono = []\n",
    "    for snp in all_snps:\n",
    "        index_snp = snp.split('_')[0]\n",
    "        beta_value = betas.loc[betas['snp'] == index_snp, 'Beta'].values[0]\n",
    "        phenos_mono.append(complete[snp] * beta_value)\n",
    "    \n",
    "    # Converting phenos_mono list of series to DataFrame directly\n",
    "    phenos_mono = pd.concat(phenos_mono, axis=1)\n",
    "    phenos_mono.columns = complete.columns\n",
    "    \n",
    "    # Add noise\n",
    "    n = len(phenos_mono)\n",
    "    for snp in list(phenos_mono.columns):\n",
    "        var_effect = np.var(phenos_mono[snp])\n",
    "        total_variance = var_effect / snp_noise\n",
    "        var_noise = total_variance - var_effect\n",
    "        sd_noise = np.sqrt(var_noise)\n",
    "        # Generate phenotype with noise\n",
    "        phenos_mono[snp] = phenos_mono[snp] + np.random.normal(0, sd_noise, n)\n",
    "        std_dev = np.std(phenos_mono[snp])\n",
    "        mean = np.mean(phenos_mono[snp])\n",
    "        phenos_mono[snp] = (phenos_mono[snp] - mean) / std_dev\n",
    "\n",
    "    if len([f for f in scenarios if f.split('_')[0]=='mix']) > 0:\n",
    "        ratio_effects.append(0.2)\n",
    "        ratio_effects.append(0.5)\n",
    "        ratio_effects.append(0.7)\n",
    "    else:\n",
    "        pass\n",
    "    if 'snp_effect' in scenarios:\n",
    "        ratio_effects.append(1)\n",
    "else:\n",
    "    ratio_effects.append(0)\n",
    "    phenos_mono = pd.DataFrame()\n",
    "    phenos_mono[complete.columns[0]] = np.zeros(len(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680571-71da-4d7b-b029-b5e906ddfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/plots/\"\n",
    "path_pvals = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/pvals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9378-cbec-4b26-81f2-a110cf1ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_plots, exist_ok=True)\n",
    "os.makedirs(path_pvals, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9399540-4c50-491b-9367-7865546e500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_noises = [0.1, 0.5]\n",
    "if 'snp_effect' in scenarios:\n",
    "    env_noises = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d554f-75ff-4ad7-8a44-60810f6f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_risks = risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f1d52-d51d-4934-b0e6-820467026666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134ad-afc3-41c9-85a7-a28e0c18caca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performances = pd.DataFrame()\n",
    "computing = pd.DataFrame()\n",
    "\n",
    "for ratio_effect in ratio_effects:\n",
    "    ratio_environment = 1-ratio_effect\n",
    "    for env_noise in env_noises:\n",
    "        for name_risk in iter_risks:\n",
    "            risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/environmental_risks/risk_{name_risk}.pkl\")\n",
    "            populations[name_risk] = risk[name_risk]\n",
    "            df_agg = populations.groupby(['x', 'y']).agg({name_risk: 'mean'}).reset_index()\n",
    "            grid_df = df_agg.pivot(index='y', columns='x', values=name_risk)\n",
    "            sns.heatmap(grid_df, cmap='rocket_r', linewidths=.5, square=True, cbar=False)\n",
    "            \n",
    "            # Add a title to the heatmap\n",
    "            plt.title(f\"{naming_dict[name_risk]}\", fontsize=16)\n",
    "            plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "            plt.savefig(f\"{path_plots}/envriskmap_{name_risk}.png\", dpi=100)\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "            y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk]), env_noise))\n",
    "            # Calculate the standard deviation and mean\n",
    "            std_dev = np.std(y)\n",
    "            mean = np.mean(y)\n",
    "            # Standardize\n",
    "            y = (y - mean) / std_dev\n",
    "            risk['pheno'] = y\n",
    "            y_snps = phenos_mono.copy()\n",
    "            for snp in list(phenos_mono.columns):\n",
    "                y_snps[snp] = phenos_mono[snp]*ratio_effect + y*ratio_environment\n",
    "            y = y_snps.copy()\n",
    "            \n",
    "            df_pvals = pd.DataFrame()\n",
    "            df_time = pd.DataFrame()\n",
    "            df_bests = pd.DataFrame()\n",
    "            \n",
    "            # Best correction\n",
    "            start_time = time.time()\n",
    "            df_best_corr = manhattan_linear(complete, y, risk[[f\"{name_risk}\"]])\n",
    "            end_time = time.time()\n",
    "            elapsed_time_best_corr = np.round(end_time - start_time,3)\n",
    "            \n",
    "            df_pvals[['snp','coefs','AFs']] = df_best_corr[['snp','coefs','AFs']]\n",
    "            df_pvals[\"-logP_best_corr\"] = df_best_corr['-logPs']\n",
    "            df_time[\"best_corr\"] = [elapsed_time_best_corr]\n",
    "            df_bests[\"-logP_best_corr\"] = np.sort(df_best_corr['-logPs'])\n",
    "            \n",
    "            n = len(df_bests)\n",
    "            expected_quantiles = np.arange(1, n + 1) / n\n",
    "            expected_logP = np.sort(-np.log10(expected_quantiles))\n",
    "            df_bests['expected_logP'] = expected_logP\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_best_corr', data=df_bests, color='black', label=f\"Best correction\", linewidth=0)\n",
    "\n",
    "            if \"no_corr\" in tools:\n",
    "                # No correction\n",
    "                start_time = time.time()\n",
    "                df_no_corr = manhattan_linear(complete, y)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_no_corr = np.round(end_time - start_time,3)\n",
    "                df_pvals['-logP_no_corr'] = df_no_corr['-logPs']\n",
    "                df_time[\"no_corr\"] = [elapsed_time_no_corr]\n",
    "                df_bests[\"-logP_no_corr\"] = np.sort(df_no_corr['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_no_corr', data=df_bests, color='red', label='no covariate', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "\n",
    "\n",
    "            if \"x_y\" in tools:\n",
    "                # X and Y axis as covaraites\n",
    "                start_time = time.time()\n",
    "                pops = populations[['x','y']]\n",
    "                df_pops = manhattan_linear(complete, y , pops)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_x_y = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_x_y\"] = df_pops['-logPs']\n",
    "                df_time[\"x_y\"] = [elapsed_time_x_y]\n",
    "                df_bests[\"-logP_x_y\"] = np.sort(df_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_x_y', data=df_bests, color='darkblue', label='X,Y axis as covariate', linewidth=0)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss_counted\" in tools:\n",
    "                # p2 - q2 and 2pq as covariates\n",
    "                start_time = time.time()\n",
    "                df_p_q_2pq_covs_via_true_pops = manhattan_linear(complete, y, true_p2s-true_q2s)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_true_ps = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_abyss_via_true_pops\"] = df_p_q_2pq_covs_via_true_pops['-logPs']\n",
    "                df_time[\"abyss_via_true_pops\"] = [elapsed_time_true_ps]\n",
    "                df_bests[\"-logP_abyss_via_true_pops\"] = np.sort(df_p_q_2pq_covs_via_true_pops['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops', data=df_bests, color='orange', label='True MAFs as covs', linewidth=0)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss_counted_corrected\" in tools:\n",
    "                # p2 - q2 and 2pq as covariates\n",
    "                X = complete.copy()\n",
    "                corr = true_p2s-true_q2s\n",
    "                #X = X - correction_counted\n",
    "                start_time = time.time()\n",
    "                df_corrected = manhattan_linear((true_p2s-true_q2s), y)\n",
    "\n",
    "                for snp in corr.columns:\n",
    "                    snp_index = snp.split('_AF_')[0]\n",
    "                    beta_value = df_corrected.loc[df_corrected['snp'] == snp_index, 'coefs'].values[0]\n",
    "                    corr[snp] = corr[snp]*beta_value\n",
    "                new_y = y - corr\n",
    "                df_p_q_2pq_covs_via_true_pops_corrected = manhattan_linear(X-(true_p2s-true_q2s), new_y)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_true_ps = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_abyss_via_true_pops_corrected\"] = df_p_q_2pq_covs_via_true_pops_corrected['-logPs']\n",
    "                df_time[\"abyss_via_true_pops_corrected\"] = [elapsed_time_true_ps]\n",
    "                df_bests[\"-logP_abyss_via_true_pops_corrected\"] = np.sort(df_p_q_2pq_covs_via_true_pops_corrected['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops_corrected', data=df_bests, color='green', label='True MAFs as covs corrected', linewidth=0)\n",
    "            else:\n",
    "                pass\n",
    "            if \"PCA\" in tools:\n",
    "                # PCs\n",
    "                start_time = time.time()\n",
    "                df_PCs = manhattan_linear(complete, y , PC_complete[pc_columns])\n",
    "                end_time = time.time()\n",
    "                elapsed_time_pcs = np.round(end_time - start_time,3)\n",
    "                df_pvals[\"-logP_PCs\"] = df_PCs['-logPs']\n",
    "                df_time[\"PCs\"] = [elapsed_time_pcs]\n",
    "                df_bests[\"-logP_PCs\"] = np.sort(df_PCs['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_PCs', data=df_bests, color='pink', label=f\"{nr_complete_PCs} PCs as covariate\", linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if \"abyss\" in tools:          \n",
    "                # Abyss\n",
    "                start_time = time.time()\n",
    "                df_abyss_p_q_2pq_covs_via_esti_pop = manhattan_linear(complete, y, esti_p2-esti_q2)            \n",
    "                end_time = time.time()\n",
    "                elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_abyss_pq2pq_via_esti_pops\"] = df_abyss_p_q_2pq_covs_via_esti_pop['-logPs']\n",
    "                df_time[\"abyss_pq2pq_via_esti_pops\"] = [elapsed_time_abyss]\n",
    "                df_bests[\"-logP_abyss_pq2pq_via_esti_pops\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops', data=df_bests, color='yellow', label='Estimates p2, q2, 2pq as covs', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if \"abyss_corrected\" in tools:          \n",
    "                # Abyss\n",
    "                X = complete.copy()\n",
    "                corr = esti_p2-esti_q2\n",
    "                start_time = time.time()\n",
    "                df_corrected = manhattan_linear(corr, y)\n",
    "                for snp in corr.columns:\n",
    "                    snp_index = snp.split('_AF_')[0]\n",
    "                    beta_value = df_corrected.loc[df_corrected['snp'] == snp_index, 'coefs'].values[0]\n",
    "                    corr[snp] = corr[snp]*beta_value\n",
    "                new_y = y - corr\n",
    "                df_abyss_p_q_2pq_covs_via_esti_pop_corrected = manhattan_linear(X-(esti_p2-esti_q2), new_y)            \n",
    "                end_time = time.time()\n",
    "                elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_abyss_pq2pq_via_esti_pops_corrected\"] = df_abyss_p_q_2pq_covs_via_esti_pop_corrected['-logPs']\n",
    "                df_time[\"abyss_pq2pq_via_esti_pops_corrected\"] = [elapsed_time_abyss]\n",
    "                df_bests[\"-logP_abyss_pq2pq_via_esti_pops_corrected\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop_corrected['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops_corrected', data=df_bests, color='blue', label='Estimates p2, q2, 2pq as covs + correction', linewidth=0)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            if \"abyss_pca\" in tools:\n",
    "                # Combines\n",
    "                start_time = time.time()\n",
    "                df_abyss_combined = manhattan_linear(complete, y, covariate_dictionary_combi)\n",
    "                end_time = time.time()\n",
    "                elapsed_time_combined = np.round(end_time - start_time,3) + total_abyss_time\n",
    "                df_pvals[\"-logP_combined\"] = df_abyss_combined['-logPs']\n",
    "                df_time[\"combined\"] = [elapsed_time_combined]\n",
    "                df_bests[\"-logP_combined\"] = np.sort(df_abyss_combined['-logPs'])\n",
    "                sns.scatterplot(x='expected_logP', y='-logP_combined', data=df_bests, color='purple', label='Combined Abyss and PCA', linewidth=0)\n",
    "\n",
    "            df_pvals.to_pickle(f\"{path_pvals}/P_vals_risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}.pkl\")\n",
    "            \n",
    "            # Find the maximum value in the DataFrame excluding inf and NaN\n",
    "            max_value = df_bests.replace([np.inf, -np.inf], np.nan).max().max()\n",
    "            \n",
    "            # Replace inf values with the maximum value found\n",
    "            df_bests.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "            \n",
    "            # Replace NaN values with the maximum value found\n",
    "            df_bests.fillna(max_value, inplace=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_differences = df_bests.subtract(df_bests['-logP_best_corr'], axis=0).abs()\n",
    "            performances[f\"risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}\"] = list(df_differences.mean(axis=0))\n",
    "            computing[f\"risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}\"] = list(df_time.T[0])\n",
    "        \n",
    "            \n",
    "            \n",
    "            # Plot diagonal reference line\n",
    "            plt.plot([min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     [min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     color='red', linestyle='--')\n",
    "            \n",
    "            # Set plot labels and title\n",
    "            plt.xlabel('Expected')\n",
    "            plt.ylabel('-Log10(P) Values')\n",
    "            plt.title(f\"QQ Plot of Log Values - {naming_dict[name_risk]},\\nSNP effect={ratio_effect*100}%, noise={np.round(snp_noise*100)}%\\n Environment={np.round(ratio_environment*100)}%, noise={env_noise}%\")\n",
    "            \n",
    "            # Show legend\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{path_plots}/qq_risk_{name_risk}_envnoise_{env_noise}_causalnoise_{snp_noise}_ratioeffect_{ratio_effect}.png\", dpi=100)\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    iter_risks = risks\n",
    "\n",
    "os.system(f\"rm -rf data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}\")\n",
    "performances['tools'] = list(df_bests.columns)\n",
    "computing['tools'] = list(df_time.columns)\n",
    "\n",
    "path_results = f\"data/results/\"\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "performances.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_{snp_noise}_performances.pkl\")\n",
    "computing.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_{snp_noise}_computingtimes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb60fb-b406-4bce-8023-bf55b2b95893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
