{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a205c-fd73-4567-9136-55bddd4e5dcc",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e94eaf-1f63-409c-8ecb-28fd5772b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b64d91-f02b-4780-94da-b1c832a55fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the environmental risks\n",
    "naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\",\n",
    "    \"gauss_blob_risk\" : \"Gaussian Risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"mid_mid_square_risk\": \"Mid square risk\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4dec87-a0fb-474a-b5e3-1da03bc47c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = list(naming_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437e37a-a68c-4732-a495-296eb155702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae459b3-c1df-46d1-ab65-fd01386b9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "k = int(dict['k'])\n",
    "M = float(dict['M'])\n",
    "\n",
    "# Thresholds\n",
    "very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "\n",
    "rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "\n",
    "common_threshold_L = float(dict['common_threshold_L'])\n",
    "common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85c00d-24af-4157-b311-bf7f4147b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f513fa-93e8-4912-a573-0b1e42173788",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = very_rare.rename(columns=lambda x: 'VR' + x)/2\n",
    "#very_rare = very_rare.iloc[::-1].reset_index(drop=True)\n",
    "rare = rare.rename(columns=lambda x: 'R' + x)/2\n",
    "common = common.rename(columns=lambda x: 'C' + x)/2\n",
    "complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "complete = ((complete*2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f37f2-2a83-4158-922c-b06772ba7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c68689-2f16-48bd-b0d8-3d011a21a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ae321-48aa-4a89-9754-6ae609c1c8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed693372-1c10-4437-9a48-7307c09e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/01_population_structure.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76429e-ea46-486a-8200-9509da4209b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/true_p2_via_true_pop.pkl\")\n",
    "true_twopqs = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/true_twopq_via_true_pop.pkl\")\n",
    "true_q2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/true_q2_via_true_pop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11f442-7a9f-46a4-82e8-898a1d3f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotpops = pd.get_dummies(populations[['populations']], columns=['populations']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca752c9-bf34-41bb-8a20-b6cd60ed9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "nr_common_PCs = 24\n",
    "pc_columns = ['PC{}'.format(i) for i in range(1, nr_common_PCs+1)]\n",
    "PC_common= pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/PCs/common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80e0c9-0d03-4e63-8d22-492daeea1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_0 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/esti_p2_zoom_0_via_esti_pop.pkl\")\n",
    "twopq_0 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/esti_2pq_zoom_0_via_esti_pop.pkl\")\n",
    "q2_0 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/esti_q2_zoom_0_via_esti_pop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41914dd0-b824-420a-959f-0966115460cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_true = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p_q\"] = true_p2s[snp] - true_q2s[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = true_twopqs[snp]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_true[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3f44c-3c25-492f-9af4-0bff4188121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_esti_0 = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p2\"] = p2_0[snp]\n",
    "    temp[f\"{snp}_cov_q2\"] = q2_0[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = twopq_0[snp]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_esti_0[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9b5fb-afac-4c49-a5c5-a7802718edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139db3f-bf78-486b-9923-18e09e32b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_combi = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p2\"] = p2_0[snp]\n",
    "    temp[f\"{snp}_cov_q2\"] = q2_0[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = twopq_0[snp]\n",
    "    temp[pc_columns] = PC_common[pc_columns]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_combi[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe5b4c-7db3-45e1-8b94-414697cba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])\n",
    "precision = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680571-71da-4d7b-b029-b5e906ddfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9378-cbec-4b26-81f2-a110cf1ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_plots, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134ad-afc3-41c9-85a7-a28e0c18caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_risk in risks:\n",
    "    risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/environmental_risks/risk_{name_risk}.pkl\")\n",
    "    populations[name_risk] = risk[name_risk]\n",
    "    df_agg = populations.groupby(['x', 'y']).agg({name_risk: 'mean'}).reset_index()\n",
    "    grid_df = df_agg.pivot(index='y', columns='x', values=name_risk)\n",
    "    sns.heatmap(grid_df, cmap='rocket_r', linewidths=.5, square=True, cbar=False)\n",
    "    \n",
    "    # Add a title to the heatmap\n",
    "    plt.title(f\"{naming_dict[name_risk]}\", fontsize=16)\n",
    "    plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "    plt.savefig(f\"{path_plots}/envriskmap_{name_risk}.png\", dpi=100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk]), precision))\n",
    "    # Calculate the standard deviation and mean\n",
    "    std_dev = np.std(y)\n",
    "    mean = np.mean(y)\n",
    "    # Standardize\n",
    "    y = (y - mean) / std_dev\n",
    "    risk['pheno'] = y\n",
    "\n",
    "    # No correction\n",
    "    df_no_corr = manhattan_linear(complete, risk[['pheno']])\n",
    "\n",
    "    # X and Y axis as covaraites\n",
    "    pops = populations[['x','y']]\n",
    "    df_pops = manhattan_linear(complete, y , pops)\n",
    "\n",
    "    # p2 - q2 and 2pq as covariates\n",
    "\n",
    "    df_p_q_2pq_covs_via_true_pops = manhattan_linear(complete, y, covariate_dictionary_true)\n",
    "#    autoencoder, bottleneck_model, history = abyss(complete, complete, bottleneck_nr, epoch, patience)\n",
    "    df_PCs = manhattan_linear(complete, y , PC_common[pc_columns])\n",
    "\n",
    "    df_abyss_p_q_2pq_covs_via_esti_pop = manhattan_linear(complete, y, covariate_dictionary_esti_0)\n",
    "    df_abyss_combined = manhattan_linear(complete, y, covariate_dictionary_combi)\n",
    "    \n",
    "    df_pops_onehot = manhattan_linear(complete, y , onehotpops)\n",
    "    \n",
    "    # Create QQ plot\n",
    "    df_bests = pd.DataFrame()\n",
    "    df_bests[\"-logP_no_corr\"] = np.sort(df_no_corr['-logPs'])\n",
    "    df_bests[\"-logP_true_pop\"] = np.sort(df_pops['-logPs'])\n",
    "    df_bests[\"-logP_true_pop_onehot\"] = np.sort(df_pops_onehot['-logPs'])\n",
    "    #df_bests[\"-logP_abyss_via_esti_pops\"] = np.sort(df_abyss_p_q_covs_via_esti_pop['-logPs'])\n",
    "    df_bests[\"-logP_abyss_via_true_pops\"] = np.sort(df_p_q_2pq_covs_via_true_pops['-logPs'])\n",
    "    df_bests[\"-logP_abyss_pq2pq_via_esti_pops\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop['-logPs'])\n",
    "    df_bests[\"-logP_combined\"] = np.sort(df_abyss_combined['-logPs'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_bests[\"-logP_PCs\"] = np.sort(df_PCs['-logPs'])\n",
    "\n",
    "    # Find the maximum value in the DataFrame excluding inf and NaN\n",
    "    max_value = df_bests.replace([np.inf, -np.inf], np.nan).max().max()\n",
    "    \n",
    "    # Replace inf values with the maximum value found\n",
    "    df_bests.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "    \n",
    "    # Replace NaN values with the maximum value found\n",
    "    df_bests.fillna(max_value, inplace=True)\n",
    "    n = len(df_bests)\n",
    "    expected_quantiles = np.arange(1, n + 1) / n\n",
    "    expected_logP = np.sort(-np.log10(expected_quantiles))\n",
    "    df_bests['expected_logP'] = expected_logP\n",
    "\n",
    "    sns.scatterplot(x='expected_logP', y='-logP_no_corr', data=df_bests, color='red', label='no covariate', linewidth=0)\n",
    "    \n",
    "    sns.scatterplot(x='expected_logP', y='-logP_true_pop', data=df_bests, color='darkblue', label='True populations x,y axis as covariate', linewidth=0)\n",
    "    \n",
    "    sns.scatterplot(x='expected_logP', y='-logP_true_pop_onehot', data=df_bests, color='lightblue', label='True populations dummies as covariates', linewidth=0)\n",
    "    \n",
    "    #sns.scatterplot(x='expected_logP', y='-logP_abyss_via_esti_pops', data=df_bests, color='purple', label='Estimated MAFs as covs', linewidth=0)\n",
    "    sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops', data=df_bests, color='orange', label='True MAFs as covs', linewidth=0)\n",
    "    sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops', data=df_bests, color='yellow', label='Estimates p2, q2, 2pq as covs', linewidth=0)\n",
    "    sns.scatterplot(x='expected_logP', y='-logP_combined', data=df_bests, color='green', label='combined', linewidth=0)\n",
    "    \n",
    "    sns.scatterplot(x='expected_logP', y='-logP_PCs', data=df_bests, color='pink', label=f\"{nr_common_PCs} PCs as covariate\", linewidth=0)\n",
    "    \n",
    "    # Plot diagonal reference line\n",
    "    plt.plot([min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "             [min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "             color='red', linestyle='--')\n",
    "    \n",
    "    # Set plot labels and title\n",
    "    plt.xlabel('Expected')\n",
    "    plt.ylabel('-Log10(P) Values')\n",
    "    plt.title(f\"QQ Plot of Log Values - {naming_dict[name_risk]}\")\n",
    "    \n",
    "    # Show legend\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{path_plots}/qq_only_env_{name_risk}.png\", dpi=100)\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4a5f2-524c-4812-9907-45b51fb36d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32a940-ee84-47b3-928f-ba4898f9805c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba893ef5-cd10-40d1-9f8c-dfbbb9a613bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdeb911-b4c0-4f8f-862c-8efbbbd2aadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
