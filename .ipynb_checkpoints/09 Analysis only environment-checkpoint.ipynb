{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c7bce4-6976-468e-bd48-9aa23eb7071a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2227d507-afd9-4bfe-9bba-a8e710b92b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 14:11:04.858123: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-28 14:11:04.906202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-28 14:11:04.906258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-28 14:11:04.907449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-28 14:11:04.916202: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-28 14:11:04.917471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-28 14:11:06.162346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, lin_reg, simulate_quant_trait\n",
    "from models import no_corr, rare_pc, pc, gc, abyss_bottle_linreg, abyss_maf_linreg\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "import warnings\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2344990-e520-4dbe-91dd-858e0ea99734",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed947-1d5c-4118-964f-11efaf4fe54b",
   "metadata": {},
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e6f3d0-5a9c-49d7-a4ce-e888e28c147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "k = int(dict['k'])\n",
    "M = float(dict['M'])\n",
    "\n",
    "# Thresholds\n",
    "very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "\n",
    "rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "\n",
    "common_threshold_L = float(dict['common_threshold_L'])\n",
    "common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f564f7-ccd1-4b33-bc3c-25de0646ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NW_risk'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_level = get_risk_level()\n",
    "risk_level = risk_level.split(\"\\n\")[-1]\n",
    "name_risk = risk_level.split('_fun')[0]\n",
    "name_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e318376e-ab9c-4d79-947e-cfb139f6cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_common_PCs = 5\n",
    "pc_columns = ['PC{}'.format(i) for i in range(1, nr_common_PCs+1)]\n",
    "nr_rare_PCs = 5\n",
    "rare_pc_columns = ['PC{}'.format(i) for i in range(1, nr_rare_PCs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54b364d-76a9-4e4e-87c5-1b8593c763d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6b8e47-23ff-419e-a6be-a32efeb00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = very_rare.rename(columns=lambda x: 'VR' + x)/2\n",
    "rare = rare.rename(columns=lambda x: 'R' + x)/2\n",
    "common = common.rename(columns=lambda x: 'C' + x)/2\n",
    "complete = pd.concat([common, rare, very_rare], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde15ba2-256e-4605-913a-d859ec2aa1da",
   "metadata": {},
   "source": [
    "# Load environmental risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5f5306-e8ff-4a74-a38d-50bc7483cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/environmental_risks/risk_{name_risk}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d54c4c-9da2-499d-b0b5-898051030418",
   "metadata": {},
   "source": [
    "# Create phenotype only ENV dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a3a247-fa0d-43c7-9798-557a22049d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])\n",
    "y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858e67c-e27d-4eaf-b2bb-29b87c774029",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2ea6c4-9953-4827-abc8-525cc38b5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No correction\n",
    "df_no_corr = no_corr(complete, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d565fb98-65d6-4176-a4ea-b7115f3e4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rare PCA\n",
    "PC_veryrare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/PCs/veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "df_rare_PCs = rare_pc(complete, y , PC_veryrare, rare_pc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8368a798-86ff-48c7-afb0-7052ad4b4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "PC_common= pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/PCs/common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "df_PCs = rare_pc(complete, y , PC_common, pc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5709e32-82bb-4de9-8e9a-be726110047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genomic control\n",
    "df_GC = gc(df_no_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6773c209-5f7c-49f4-adde-2f963c94d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimalist abyss\n",
    "bottleneck_nr = 2\n",
    "epoch = 5\n",
    "patience = 2\n",
    "dim_columns = ['dim{}'.format(i) for i in range(1, bottleneck_nr+1)]\n",
    "autoencoder, bottleneck_model, history = abyss(complete*2 - 1, bottleneck_nr, epoch, patience)\n",
    "\n",
    "abyss_bottle = bottleneck_model(tf.convert_to_tensor(complete*2 - 1, dtype=tf.float32))\n",
    "abyss_MAF = autoencoder(tf.convert_to_tensor(complete*2 - 1, dtype=tf.float32))\n",
    "probmaf = (pd.DataFrame(data=abyss_MAF, columns = complete.columns)+1)/2\n",
    "\n",
    "df_abyss_bottle = abyss_bottle_linreg(complete, y, abyss_bottle)\n",
    "df_abyss_maf = abyss_maf_linreg(complete, y, probmaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e30492-2686-4f01-8cd2-701faad1a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep abyss\n",
    "def deep_abyss(geno, bottle, epoch, patience, pheno):\n",
    "    dim_columns = ['dim{}'.format(i) for i in range(1, bottle+1)]\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test, pheno_train, pheno_test = train_test_split(geno, geno, pheno, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define your regularization strength (lambda)\n",
    "    l2_lambda = 0.001  # Adjust this value as needed\n",
    "\n",
    "    # Define input layers\n",
    "    input_shape_geno = geno.shape[1:]\n",
    "    input_layer_geno = Input(shape=input_shape_geno, name='input_geno')\n",
    "\n",
    "    input_shape_pheno = pheno.shape[1:]\n",
    "    input_layer_pheno = Input(shape=input_shape_pheno, name='input_pheno')\n",
    "\n",
    "    # Define bottleneck size\n",
    "\n",
    "    size_layer_1 = int(round(input_shape_geno[0]) / 2)\n",
    "\n",
    "    # Create layers\n",
    "    encoder_init_1 = layers.Dense(bottle, \n",
    "                           activation=\"elu\", \n",
    "                           name=\"encoder_init_1\",\n",
    "                           kernel_regularizer=regularizers.l2(l2_lambda))\n",
    "    \n",
    "    decoder_init_2 = layers.Dense(input_shape_geno[0], \n",
    "                           activation=\"elu\", \n",
    "                           name=\"decoder_init_2\",\n",
    "                           kernel_regularizer=regularizers.l2(l2_lambda))\n",
    "    \n",
    "    predictor = layers.Dense(input_shape_pheno[0], \n",
    "                           activation=\"linear\", \n",
    "                           name=\"predictor\",\n",
    "                           kernel_regularizer=regularizers.l2(l2_lambda))\n",
    "\n",
    "    # Define custom layer for element-wise trainable weights\n",
    "    class ElementWiseWeightsLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(ElementWiseWeightsLayer, self).__init__(**kwargs)\n",
    "    \n",
    "        def build(self, input_shape):\n",
    "            self.weight = self.add_weight(shape=(), initializer=\"ones\", trainable=True, name=\"element_wise_weight\")\n",
    "            super(ElementWiseWeightsLayer, self).build(input_shape)\n",
    "    \n",
    "        def call(self, inputs):\n",
    "            return inputs * self.weight\n",
    "    \n",
    "    # Define encoder and decoder paths\n",
    "    bottle_neck = encoder_init_1(input_layer_geno)\n",
    "    allele_frequency_probability = decoder_init_2(bottle_neck)\n",
    "    y_predictor = predictor(allele_frequency_probability)\n",
    "    \n",
    "    # Define the model\n",
    "    autoencoder = Model(inputs=input_layer_geno, outputs=[allele_frequency_probability, y_predictor], name=\"fishy\")\n",
    "    # Extract the bottleneck layer\n",
    "    bottleneck_model = tf.keras.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder_init_1').output)\n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss=['mse', 'mse'], loss_weights=[1.0, 2.0])\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = autoencoder.fit(X_train, [X_train, pheno_train], epochs=epochs, batch_size=32, validation_data=(X_test, [X_test, pheno_test]), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluation = autoencoder.evaluate(X_test, [y_test, pheno_test])\n",
    "    \n",
    "    # Predict outputs\n",
    "    allele_frequency_output, y_output = autoencoder.predict(geno)\n",
    "    \n",
    "    # Extract encoded outputs from the encoder (bottleneck layer)\n",
    "    encoded_output = bottleneck_model.predict(geno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f97f10ab-e7b6-46aa-9d9e-bf9043619b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coeff</th>\n",
       "      <th>AFs</th>\n",
       "      <th>Ps_abyss_maf</th>\n",
       "      <th>expected_P</th>\n",
       "      <th>logPs_abyss_maf</th>\n",
       "      <th>expected_logP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.064727</td>\n",
       "      <td>0.296425</td>\n",
       "      <td>7.446390e-08</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.21885</td>\n",
       "      <td>7.105215e-06</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000402</td>\n",
       "      <td>0.21275</td>\n",
       "      <td>1.744526e-05</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.001008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.017530</td>\n",
       "      <td>0.2368</td>\n",
       "      <td>2.568836e-05</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.001512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.217525</td>\n",
       "      <td>4.668449e-05</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>0.059531</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>9.929324e-01</td>\n",
       "      <td>0.995365</td>\n",
       "      <td>4.330827</td>\n",
       "      <td>2.237041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>-0.170837</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>9.955195e-01</td>\n",
       "      <td>0.996524</td>\n",
       "      <td>4.590264</td>\n",
       "      <td>2.333951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.158448</td>\n",
       "      <td>0.008</td>\n",
       "      <td>9.955583e-01</td>\n",
       "      <td>0.997683</td>\n",
       "      <td>4.758323</td>\n",
       "      <td>2.458890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>-0.162501</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>9.960850e-01</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>5.148423</td>\n",
       "      <td>2.634981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0.010129</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>9.991179e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.128054</td>\n",
       "      <td>2.936011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>863 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        coeff       AFs  Ps_abyss_maf  expected_P  logPs_abyss_maf  \\\n",
       "0   -0.064727  0.296425  7.446390e-08    0.001159         0.000383   \n",
       "1    0.007621   0.21885  7.105215e-06    0.002317         0.001704   \n",
       "2   -0.000402   0.21275  1.744526e-05    0.003476         0.001933   \n",
       "3   -0.017530    0.2368  2.568836e-05    0.004635         0.001950   \n",
       "4    0.003380  0.217525  4.668449e-05    0.005794         0.003080   \n",
       "..        ...       ...           ...         ...              ...   \n",
       "858  0.059531  0.016075  9.929324e-01    0.995365         4.330827   \n",
       "859 -0.170837    0.0108  9.955195e-01    0.996524         4.590264   \n",
       "860 -0.158448     0.008  9.955583e-01    0.997683         4.758323   \n",
       "861 -0.162501  0.010525  9.960850e-01    0.998841         5.148423   \n",
       "862  0.010129   0.00345  9.991179e-01    1.000000         7.128054   \n",
       "\n",
       "     expected_logP  \n",
       "0        -0.000000  \n",
       "1         0.000504  \n",
       "2         0.001008  \n",
       "3         0.001512  \n",
       "4         0.002018  \n",
       "..             ...  \n",
       "858       2.237041  \n",
       "859       2.333951  \n",
       "860       2.458890  \n",
       "861       2.634981  \n",
       "862       2.936011  \n",
       "\n",
       "[863 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4890506-55ae-4cc7-9a3f-31615015f806",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeep_abyss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottleneck_nr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrisk\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname_risk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m, in \u001b[0;36mdeep_abyss\u001b[0;34m(geno, bottle, epoch, patience, pheno)\u001b[0m\n\u001b[1;32m      3\u001b[0m dim_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, bottle\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, y_train, y_test, pheno_train, pheno_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(geno, geno, pheno, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define your regularization strength (lambda)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m l2_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Adjust this value as needed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "deep_abyss(complete*2 - 1, bottleneck_nr, epoch, patience, risk[[name_risk]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59427bd4-a44e-4a8a-86e4-6ae487579607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
