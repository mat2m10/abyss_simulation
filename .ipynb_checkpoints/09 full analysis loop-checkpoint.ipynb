{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a205c-fd73-4567-9136-55bddd4e5dcc",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e94eaf-1f63-409c-8ecb-28fd5772b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import time  # Import the time module\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b64d91-f02b-4780-94da-b1c832a55fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4dec87-a0fb-474a-b5e3-1da03bc47c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = list(naming_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b437e37a-a68c-4732-a495-296eb155702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae459b3-c1df-46d1-ab65-fd01386b9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "    \n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "HWE = int(dict['HWE'])\n",
    "\n",
    "nr_humans = int(dict['nr_humans'])\n",
    "nr_snps = int(dict['nr_snps'])\n",
    "bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['PCA', 'abyss_counted', 'abyss', 'no_corr']\n",
    "\n",
    "\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k\n",
    "\n",
    "\n",
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "\n",
    "if 'to_analyze' not in globals():\n",
    "    to_analyze = \"complete\"\n",
    "\n",
    "if to_analyze == \"complete\":\n",
    "    complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "elif to_analyze == \"common\":\n",
    "    complete = common.copy()\n",
    "    \n",
    "elif to_analyze == \"rare\":\n",
    "    complete = rare.copy()\n",
    "elif to_analyze == \"very_rare\":\n",
    "    complete = very_rare.copy()\n",
    "    \n",
    "complete = ((complete*2)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177b36e-9211-4628-b0fd-7579c724a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the environmental risks\n",
    "naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"N_risk\" : \"Smooth linear North environmental risk\",\n",
    "    \"blob_risk\": \"Localised big blob risk\",\n",
    "    \"center_risk\": \"Localised big central risk\",\n",
    "    \"big_square_risk\": \"big square risk\",\n",
    "    \"square_risk\" : \"Tiny square risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "    \"two_square_risk\": \"Two tiny risks\",\n",
    "    \"gauss_blob_risk\" : \"Gaussian Risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"mid_mid_square_risk\": \"Mid square risk\"\n",
    "}\n",
    "\n",
    "# Names of the environmental risks\n",
    "naming_dict = {\n",
    "    \"no_risk\": \"no environmental risk\",\n",
    "    \"NW_risk\": \"Smooth linear North-West environmental risk\",\n",
    "    \"blob_risk\": \"Localised big blob risk\",\n",
    "    'hi_square_risk' : \"Tiny square risk\",\n",
    "    \"three_square_risk\": \"Three localized Risks\",\n",
    "    \"hi_gauss_blob_risk\": \"Global Gaussian Risk\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31f37f2-2a83-4158-922c-b06772ba7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c68689-2f16-48bd-b0d8-3d011a21a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npopulations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\\npopulations[\\'population_number\\'] = populations[\\'populations\\'].str.extract(\\'(\\\\d+)\\').astype(int)\\n# Calculating X and Y coordinates\\npopulations[\\'x\\'] = ((populations[\\'population_number\\'] - 1) % k) + 1\\npopulations[\\'y\\'] = ((populations[\\'population_number\\'] - 1) // k) + 1\\npopulations[\\'z\\'] = 0.5\\npopulations[\\'population\\'] = populations[\\'population_number\\']/(k*k)\\npalette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations[\\'x\\'], populations[\\'y\\'], populations[\\'z\\'])]\\n\\n# Standardize the vector\\nstd_dev = np.std(populations[\\'x\\'])\\nmean = np.mean(populations[\\'x\\'])\\npopulations[\\'x\\'] = np.round((populations[\\'x\\'] - mean) / std_dev,2)\\n\\nstd_dev = np.std(populations[\\'y\\'])\\nmean = np.mean(populations[\\'y\\'])\\npopulations[\\'y\\'] = np.round((populations[\\'y\\'] - mean) / std_dev,2)\\n\\n# Check the grid\\ndf_agg = populations.groupby([\\'x\\', \\'y\\']).agg({\\'population\\': \\'mean\\'}).reset_index()\\n\\n# Now, pivot the aggregated DataFrame\\ngrid_df = df_agg.pivot(index=\\'y\\', columns=\\'x\\', values=\\'population\\')\\n\\n\\nheatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\\n\\n# Add a title to the heatmap\\nplt.title(\\'Population Grid\\', fontsize=16)\\nplt.gca().invert_yaxis()  # Sometimes it\\'s necessary to invert the y-axis for correct orientation\\n#plt.show()\\nplt.close()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed693372-1c10-4437-9a48-7307c09e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/01_population_structure.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c76429e-ea46-486a-8200-9509da4209b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_p2_via_true_pop.pkl\")\n",
    "true_twopqs = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_twopq_via_true_pop.pkl\")\n",
    "true_q2s = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/02_true_q2_via_true_pop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f11f442-7a9f-46a4-82e8-898a1d3f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotpops = pd.get_dummies(populations[['populations']], columns=['populations']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca752c9-bf34-41bb-8a20-b6cd60ed9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "nr_complete_PCs = 8\n",
    "pc_columns = ['PC{}'.format(i) for i in range(1, nr_complete_PCs+1)]\n",
    "\n",
    "nr_complete_PCs_abyss = 8\n",
    "pc_columns_abyss = ['PC{}'.format(i) for i in range(1, nr_complete_PCs_abyss+1)]\n",
    "#PC_common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")\n",
    "PC_complete = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/PCs/complete_genotypes_AF_0_0.5_.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b79d23c-e1e3-46fe-a8a5-9c62932daabf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pq_file \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/G\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mG\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_L\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_c\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_k\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_M\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mM\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_HWE\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHWE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/genotype/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mestimated_2pqs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m p2_file \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_HWE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHWE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_p2s\u001b[39m\u001b[38;5;124m\"\u001b[39m)][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m q2_file \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_HWE\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHWE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimated_q2s\u001b[39m\u001b[38;5;124m\"\u001b[39m)][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pq_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_2pqs\")][0]\n",
    "p2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_p2s\")][0]\n",
    "q2_file = [f for f in os.listdir(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/\") if f.startswith(\"estimated_q2s\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80e0c9-0d03-4e63-8d22-492daeea1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "esti_p2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{p2_file}\")\n",
    "esti_twopq = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{pq_file}\")\n",
    "esti_q2 = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/{q2_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bba0d-c94c-42f0-901b-36cca841a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_p2 = float(p2_file.split('pop_')[1].split('seconds')[0])\n",
    "time_q2 = float(q2_file.split('pop_')[1].split('seconds')[0])\n",
    "time_pq = float(pq_file.split('pop_')[1].split('seconds')[0])\n",
    "total_abyss_time = np.round(time_p2+time_q2+time_pq,3)\n",
    "total_abyss_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41914dd0-b824-420a-959f-0966115460cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_true = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p_q\"] = true_p2s[snp] - true_q2s[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = true_twopqs[snp]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_true[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3f44c-3c25-492f-9af4-0bff4188121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_esti_0 = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "    temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_esti_0[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139db3f-bf78-486b-9923-18e09e32b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_dictionary_combi = {}\n",
    "for snp in list(complete.columns):\n",
    "    temp = complete[[snp]].copy()\n",
    "    temp[f\"{snp}_cov_p2\"] = esti_p2[snp]\n",
    "    temp[f\"{snp}_cov_q2\"] = esti_q2[snp]\n",
    "    temp[f\"{snp}_cov_2pq\"] = esti_twopq[snp]\n",
    "    temp[pc_columns_abyss] = PC_complete[pc_columns_abyss]\n",
    "    temp = temp.drop(columns=[snp])\n",
    "    covariate_dictionary_combi[snp] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe5b4c-7db3-45e1-8b94-414697cba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu= np.zeros(complete.shape[0])\n",
    "beta = np.zeros(complete.shape[1])\n",
    "noise = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385992e0-684b-4844-a371-123c081a6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(arr, effectsize):\n",
    "    return [1 * (num + 0.001) * effectsize for num in arr]\n",
    "\n",
    "def divi(arr, effectsize):\n",
    "    return [(1 / (num + 0.001)) * effectsize for num in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a777d8-4948-4d26-94a9-c6fc885e7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "effectsize = 1\n",
    "numbers_af = [float(col.split('_AF_')[1]) for col in common.columns if '_AF_' in col]\n",
    "snp_names = [col.split('_AF_')[0] for col in common.columns if '_AF_' in col]\n",
    "\n",
    "beta_values = divi(numbers_af, effectsize)\n",
    "data = {'snp': snp_names, 'Beta': beta_values}\n",
    "beta_common = pd.DataFrame(data)\n",
    "beta_common['maf'] = \"common\"\n",
    "\n",
    "numbers_af = [float(col.split('_AF_')[1]) for col in rare.columns if '_AF_' in col]\n",
    "snp_names = [col.split('_AF_')[0] for col in rare.columns if '_AF_' in col]\n",
    "\n",
    "beta_values = divi(numbers_af, effectsize)\n",
    "data = {'snp': snp_names, 'Beta': beta_values}\n",
    "beta_rare = pd.DataFrame(data)\n",
    "beta_rare['maf'] = \"rare\"\n",
    "\n",
    "numbers_af = [float(col.split('_AF_')[1]) for col in very_rare.columns if '_AF_' in col]\n",
    "snp_names = [col.split('_AF_')[0] for col in very_rare.columns if '_AF_' in col]\n",
    "\n",
    "beta_values = divi(numbers_af, effectsize)\n",
    "data = {'snp': snp_names, 'Beta': beta_values}\n",
    "beta_very_rare = pd.DataFrame(data)\n",
    "beta_very_rare['maf'] = \"very rare\"\n",
    "\n",
    "betas = pd.concat([beta_common, beta_rare, beta_very_rare], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81ac9b-fa7b-48b4-a76c-b975853e71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snps = list(complete.columns)\n",
    "phenos_mono = []\n",
    "for snp in all_snps:\n",
    "    index_snp = snp.split('_')[0]\n",
    "    beta_value = betas.loc[betas['snp'] == index_snp, 'Beta'].values[0]\n",
    "    phenos_mono.append(complete[snp] * beta_value)\n",
    "\n",
    "# Converting phenos_mono list of series to DataFrame directly\n",
    "phenos_mono = pd.concat(phenos_mono, axis=1)\n",
    "phenos_mono.columns = complete.columns\n",
    "\n",
    "# Add noise\n",
    "n = len(phenos_mono)\n",
    "for snp in list(phenos_mono.columns):\n",
    "    var_effect = np.var(phenos_mono[snp])\n",
    "    total_variance = var_effect / 0.001\n",
    "    var_noise = total_variance - var_effect\n",
    "    sd_noise = np.sqrt(var_noise)\n",
    "    # Generate phenotype with noise\n",
    "    phenos_mono[snp] = phenos_mono[snp] + np.random.normal(0, sd_noise, n)\n",
    "    std_dev = np.std(phenos_mono[snp])\n",
    "    mean = np.mean(phenos_mono[snp])\n",
    "    phenos_mono[snp] = (phenos_mono[snp] - mean) / std_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f680571-71da-4d7b-b029-b5e906ddfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/plots/\"\n",
    "path_pvals = f\"data/results/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/pvals/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d9378-cbec-4b26-81f2-a110cf1ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path_plots, exist_ok=True)\n",
    "os.makedirs(path_pvals, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03a19d-b33f-40f0-8884-c8172ca792e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_effects = [0, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9399540-4c50-491b-9367-7865546e500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = [0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d554f-75ff-4ad7-8a44-60810f6f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_risks = risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909134ad-afc3-41c9-85a7-a28e0c18caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = pd.DataFrame()\n",
    "computing = pd.DataFrame()\n",
    "\n",
    "for ratio_effect in ratio_effects:\n",
    "    ratio_environment = 1-ratio_effect\n",
    "    if ratio_effect == 1:\n",
    "        iter_risks = ['no_risk']\n",
    "    for noise in noises:\n",
    "        for name_risk in iter_risks:\n",
    "            risk = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/environmental_risks/risk_{name_risk}.pkl\")\n",
    "            populations[name_risk] = risk[name_risk]\n",
    "            df_agg = populations.groupby(['x', 'y']).agg({name_risk: 'mean'}).reset_index()\n",
    "            grid_df = df_agg.pivot(index='y', columns='x', values=name_risk)\n",
    "            sns.heatmap(grid_df, cmap='rocket_r', linewidths=.5, square=True, cbar=False)\n",
    "            \n",
    "            # Add a title to the heatmap\n",
    "            plt.title(f\"{naming_dict[name_risk]}\", fontsize=16)\n",
    "            plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "            plt.savefig(f\"{path_plots}/envriskmap_{name_risk}.png\", dpi=100)\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "            y = np.array(simulate_quant_trait(mu, np.array(complete), beta, np.array(risk[name_risk]), noise))\n",
    "            # Calculate the standard deviation and mean\n",
    "            std_dev = np.std(y)\n",
    "            mean = np.mean(y)\n",
    "            # Standardize\n",
    "            y = (y - mean) / std_dev\n",
    "            risk['pheno'] = y\n",
    "            y_snps = phenos_mono.copy()\n",
    "            for snp in list(phenos_mono.columns):\n",
    "                y_snps[snp] = phenos_mono[snp]*ratio_effect + y*ratio_environment\n",
    "            y = y_snps.copy()\n",
    "\n",
    "            # No correction\n",
    "            start_time = time.time()\n",
    "            df_no_corr = manhattan_linear(complete, y)\n",
    "            end_time = time.time()\n",
    "            elapsed_time_no_corr = np.round(end_time - start_time,3)\n",
    "\n",
    "            # Best correction\n",
    "            start_time = time.time()\n",
    "            df_best_corr = manhattan_linear(complete, y, risk[[f\"{name_risk}\"]])\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time_best_corr = np.round(end_time - start_time,3)\n",
    "\n",
    "            \n",
    "            # X and Y axis as covaraites\n",
    "            start_time = time.time()\n",
    "            pops = populations[['x','y']]\n",
    "            df_pops = manhattan_linear(complete, y , pops)    \n",
    "            end_time = time.time()\n",
    "            elapsed_time_x_y = np.round(end_time - start_time,3)\n",
    "\n",
    "        \n",
    "            # p2 - q2 and 2pq as covariates\n",
    "            start_time = time.time()\n",
    "            df_p_q_2pq_covs_via_true_pops = manhattan_linear(complete, y, covariate_dictionary_true)\n",
    "            end_time = time.time()\n",
    "            elapsed_time_true_ps = np.round(end_time - start_time,3)\n",
    "    \n",
    "\n",
    "            # PCs\n",
    "            start_time = time.time()\n",
    "            df_PCs = manhattan_linear(complete, y , PC_complete[pc_columns])\n",
    "            end_time = time.time()\n",
    "            elapsed_time_pcs = np.round(end_time - start_time,3)\n",
    "\n",
    "            # Abyss\n",
    "            start_time = time.time()\n",
    "            df_abyss_p_q_2pq_covs_via_esti_pop = manhattan_linear(complete, y, covariate_dictionary_esti_0)            \n",
    "            end_time = time.time()\n",
    "            elapsed_time_abyss = np.round(end_time - start_time,3) + total_abyss_time\n",
    "            \n",
    "            # Combines\n",
    "            start_time = time.time()\n",
    "            df_abyss_combined = manhattan_linear(complete, y, covariate_dictionary_combi)\n",
    "            end_time = time.time()\n",
    "            elapsed_time_combined = np.round(end_time - start_time,3) + total_abyss_time\n",
    "            \n",
    "            #df_pops_onehot = manhattan_linear(complete, y , onehotpops)\n",
    "            # Create MAF & Pval df\n",
    "            df_pvals = pd.DataFrame()\n",
    "            df_pvals[['snp','coefs','AFs']] = df_no_corr[['snp','coefs','AFs']]\n",
    "            df_pvals['-logPs'] = df_no_corr['-logPs']\n",
    "            df_pvals[\"-logP_no_corr\"] = df_no_corr['-logPs']\n",
    "            df_pvals[\"-logP_best_corr\"] = df_best_corr['-logPs']\n",
    "            \n",
    "            df_pvals[\"-logP_true_pop\"] = df_pops['-logPs']\n",
    "            #df_pvals[\"-logP_true_pop_onehot\"] = df_pops_onehot['-logPs']\n",
    "            df_pvals[\"-logP_abyss_via_true_pops\"] = df_p_q_2pq_covs_via_true_pops['-logPs']\n",
    "            df_pvals[\"-logP_abyss_pq2pq_via_esti_pops\"] = df_abyss_p_q_2pq_covs_via_esti_pop['-logPs']\n",
    "            df_pvals[\"-logP_combined\"] = df_abyss_combined['-logPs']\n",
    "            df_pvals[\"-logP_PCs\"] = df_PCs['-logPs']\n",
    "            \n",
    "            # Create QQ plot\n",
    "            df_bests = pd.DataFrame()\n",
    "            df_bests[\"-logP_no_corr\"] = np.sort(df_no_corr['-logPs'])\n",
    "            df_bests[\"-logP_best_corr\"] = np.sort(df_best_corr['-logPs'])\n",
    "            \n",
    "            df_bests[\"-logP_true_pop\"] = np.sort(df_pops['-logPs'])\n",
    "            #df_bests[\"-logP_true_pop_onehot\"] = np.sort(df_pops_onehot['-logPs'])\n",
    "            df_bests[\"-logP_abyss_via_true_pops\"] = np.sort(df_p_q_2pq_covs_via_true_pops['-logPs'])\n",
    "            df_bests[\"-logP_abyss_pq2pq_via_esti_pops\"] = np.sort(df_abyss_p_q_2pq_covs_via_esti_pop['-logPs'])\n",
    "            df_bests[\"-logP_combined\"] = np.sort(df_abyss_combined['-logPs'])\n",
    "            df_bests[\"-logP_PCs\"] = np.sort(df_PCs['-logPs'])\n",
    "            \n",
    "            # Create time df\n",
    "            df_time = pd.DataFrame()\n",
    "            df_time[\"no_corr\"] = [elapsed_time_no_corr]\n",
    "            df_time[\"best_corr\"] = [elapsed_time_best_corr]\n",
    "            df_time[\"true_pop\"] = [elapsed_time_x_y]\n",
    "            #df_time[\"true_pop_onehot\"] = np.sort(df_pops_onehot['-logPs'])\n",
    "            df_time[\"abyss_via_true_pops\"] = [elapsed_time_true_ps]\n",
    "            df_time[\"abyss_pq2pq_via_esti_pops\"] = [elapsed_time_abyss]\n",
    "            df_time[\"combined\"] = [elapsed_time_combined] \n",
    "            df_time[\"PCs\"] = [elapsed_time_pcs]\n",
    "        \n",
    "            # Find the maximum value in the DataFrame excluding inf and NaN\n",
    "            max_value = df_bests.replace([np.inf, -np.inf], np.nan).max().max()\n",
    "            \n",
    "            # Replace inf values with the maximum value found\n",
    "            df_bests.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "            \n",
    "            # Replace NaN values with the maximum value found\n",
    "            df_bests.fillna(max_value, inplace=True)\n",
    "            n = len(df_bests)\n",
    "            expected_quantiles = np.arange(1, n + 1) / n\n",
    "            expected_logP = np.sort(-np.log10(expected_quantiles))\n",
    "            df_bests['expected_logP'] = expected_logP\n",
    "            df_pvals.to_pickle(f\"{path_pvals}/P_vals_risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}.pkl\")\n",
    "            \n",
    "            \n",
    "            df_differences = df_bests.subtract(df_bests['-logP_best_corr'], axis=0).abs()\n",
    "            performances[f\"risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}\"] = list(df_differences.mean(axis=0))\n",
    "            computing[f\"risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}\"] = list(df_time.T[0])\n",
    "        \n",
    "            sns.scatterplot(x='expected_logP', y='-logP_no_corr', data=df_bests, color='red', label='no covariate', linewidth=0)\n",
    "            \n",
    "            sns.scatterplot(x='expected_logP', y='-logP_true_pop', data=df_bests, color='darkblue', label='True populations x,y axis as covariate', linewidth=0)\n",
    "            \n",
    "            #sns.scatterplot(x='expected_logP', y='-logP_true_pop_onehot', data=df_bests, color='lightblue', label='True populations dummies as covariates', linewidth=0)\n",
    "            \n",
    "            #sns.scatterplot(x='expected_logP', y='-logP_abyss_via_esti_pops', data=df_bests, color='purple', label='Estimated MAFs as covs', linewidth=0)\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_abyss_via_true_pops', data=df_bests, color='orange', label='True MAFs as covs', linewidth=0)\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_abyss_pq2pq_via_esti_pops', data=df_bests, color='yellow', label='Estimates p2, q2, 2pq as covs', linewidth=0)\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_combined', data=df_bests, color='green', label='combined', linewidth=0)\n",
    "            \n",
    "            sns.scatterplot(x='expected_logP', y='-logP_PCs', data=df_bests, color='pink', label=f\"{nr_complete_PCs} PCs as covariate\", linewidth=0)\n",
    "            sns.scatterplot(x='expected_logP', y='-logP_best_corr', data=df_bests, color='black', label=f\"Best correction\", linewidth=0)\n",
    "            \n",
    "            # Plot diagonal reference line\n",
    "            plt.plot([min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     [min(df_bests['expected_logP']), max(df_bests['expected_logP'])], \n",
    "                     color='red', linestyle='--')\n",
    "            \n",
    "            # Set plot labels and title\n",
    "            plt.xlabel('Expected')\n",
    "            plt.ylabel('-Log10(P) Values')\n",
    "            plt.title(f\"QQ Plot of Log Values - {naming_dict[name_risk]}, causal/env = {ratio_effect}, phenotypic noise = {noise}\")\n",
    "            \n",
    "            # Show legend\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{path_plots}/qq_risk_{name_risk}_noise_{noise}_ratioeffect_{ratio_effect}.png\", dpi=100)\n",
    "            # Show plot\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "    iter_risks = risks\n",
    "\n",
    "os.system(f\"rm -rf data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}\")\n",
    "performances['tools'] = list(df_bests.columns)\n",
    "computing['tools'] = list(df_time.columns)\n",
    "\n",
    "path_results = f\"data/results/\"\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "performances.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_total_{total_abyss_time}seconds.pkl\")\n",
    "computing.to_pickle(f\"{path_results}/{to_analyze}_geno_G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}_computingtimes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b25440-1b38-45d8-b536-809a622ee038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e3805-34e3-417c-b6cb-29f5504540c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
