{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58340ac8-a823-4774-a293-5a12cba212c0",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003c2c09-d157-4fb7-a81b-335d397d9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import random\n",
    "import importlib.util\n",
    "\n",
    "from collections import Counter\n",
    "from math import floor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats import t, entropy, stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Input, Model, layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "from helpers import (\n",
    "    parse_variables, get_risk_level, hi_gauss_blob_risk_fun, blob_risk_fun, \n",
    "    NW_risk_fun, square_risk_fun, map_to_color, simulate_quant_trait\n",
    ")\n",
    "\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4fe17f-3e8c-4c03-a363-c9da63f14666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "if 'G' not in globals():\n",
    "    G = int(dict['G'])\n",
    "if 'L' not in globals():\n",
    "    L = int(dict['L'])\n",
    "if 'c' not in globals():\n",
    "    c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "if 'HWE' not in globals():\n",
    "    HWE = int(dict['HWE'])\n",
    "\n",
    "if 'bottleneck_nr' not in globals():\n",
    "    bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "if 'nr_humans' not in globals():\n",
    "    nr_humans = int(dict['nr_humans'])\n",
    "\n",
    "if 'nr_snps' not in globals():\n",
    "    nr_snps = int(dict['nr_snps'])\n",
    "\n",
    "if 'epoch' not in globals():\n",
    "    epoch = 500\n",
    "if 'patience' not in globals():\n",
    "    patience = 100\n",
    "\n",
    "if 'tools' not in globals():\n",
    "    tools = ['PCA', 'abyss_counted', 'abyss', 'no_corr']\n",
    "\n",
    "\n",
    "if 'scenarios' not in globals():\n",
    "    scenarios = ['snp_effect',\n",
    "                 'linear_continuous',\n",
    "                 'non_linear_continuous',\n",
    "                 'discrete_global',\n",
    "                 'discrete_localized',\n",
    "                 'mix_linear_continuous',\n",
    "                 'mix_non_linear_continuous',\n",
    "                 'mix_discrete_global',\n",
    "                 'mix_discrete_localized']\n",
    "\n",
    "if 'very_rare_threshold_L' not in globals():\n",
    "    very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "if 'very_rare_threshold_H' not in globals():\n",
    "    very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "if 'rare_threshold_L' not in globals():\n",
    "    rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "if 'rare_threshold_H' not in globals():\n",
    "    rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "if 'common_threshold_L' not in globals():\n",
    "    common_threshold_L = float(dict['common_threshold_L'])\n",
    "if 'common_threshold_H' not in globals():\n",
    "    common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = int((G*L)/2) # one loci per chromosome\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24051924-4ec5-4766-9996-dc80d65aab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb79f4b-991f-4382-b57d-a5ce05eb59a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43208c7-06c0-4c6a-b3b0-fd72f849cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "complete = ((complete*2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1fe8674-3053-479c-8323-a0a3f588be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5be8f9-959a-401f-af7b-7b3a9082af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'abyss' in tools:\n",
    "    path_bottle = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/abyss_bottleneck\"\n",
    "    bottle_file = [f for f in os.listdir(path_bottle) if int(f.split(\"_\")[2]) ==  bottleneck_nr][0]\n",
    "    elapsed_time_bottleneck = float(bottle_file.split('_')[3].split('seconds')[0])\n",
    "    bottle = pd.read_pickle(f\"{path_bottle}/{bottle_file}\")\n",
    "    complete['pop'] = bottle['cluster']\n",
    "\n",
    "\n",
    "    for pop in bottle['cluster'].unique():\n",
    "        temp_complete = complete[complete['pop']==pop]\n",
    "        temp_complete = temp_complete.drop('pop', axis = 1)\n",
    "        genos = temp_complete.T\n",
    "        sample_size = nr_snps\n",
    "        n_components = 15\n",
    "        num_clus = round(genos.shape[0] / sample_size)\n",
    "        if num_clus == 0:\n",
    "            num_clus = 1\n",
    "        else:\n",
    "            pass\n",
    "        size_clus = int(genos.shape[0]/num_clus)\n",
    "        size_min = size_clus - round(size_clus / 5)\n",
    "        size_max = size_clus + round(size_clus / 5)\n",
    "        if size_max > genos.shape[0]:\n",
    "            size_max = None\n",
    "        # Calculate the number of clusters based on sample size\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(genos)\n",
    "    \n",
    "        # Perform PCA with n components\n",
    "        pca = PCA(n_components=n_components)\n",
    "        principal_components = pca.fit_transform(df_scaled)\n",
    "        \n",
    "        # Create a new DataFrame to store the principal components\n",
    "        pc_columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "        df_pca = pd.DataFrame(data=principal_components, columns=pc_columns)\n",
    "        \n",
    "        # Apply constrained K-Means clustering\n",
    "        clf = KMeansConstrained(\n",
    "            n_clusters=num_clus,\n",
    "            size_min=size_min,\n",
    "            size_max=size_max,\n",
    "            random_state=0\n",
    "        )\n",
    "        clf.fit_predict(np.array(df_pca))\n",
    "    \n",
    "        # Assign cluster labels to genotypic data\n",
    "        count = Counter(clf.labels_)\n",
    "        empty = []\n",
    "        sample_size_temp = floor(len(clf.labels_)/sample_size)+1\n",
    "        for val in count.values():\n",
    "            multi = floor(val/sample_size_temp)+1\n",
    "            list_to_sample = multi*list(range(sample_size_temp))\n",
    "            random.shuffle(list_to_sample)\n",
    "            empty = empty + list_to_sample[0:val]\n",
    "            \n",
    "        genos['clusters_k_means'] = empty # shuffled\n",
    "        #genos['clusters_k_means'] = clf.labels_ # clustered\n",
    "    \n",
    "        PATH_output = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks/{pop}\"\n",
    "        os.system(f\"rm -rf {PATH_output}\")\n",
    "        os.makedirs(PATH_output, exist_ok=True)\n",
    "    \n",
    "        for num in genos.clusters_k_means.unique():\n",
    "            to_save = genos.loc[genos['clusters_k_means'] == num]\n",
    "            to_save = to_save.drop(columns=['clusters_k_means']).T\n",
    "            to_save.index = temp_complete.index\n",
    "            # Calculate minimum and maximum MAF for the cluster\n",
    "            tot_mafs = []\n",
    "            for snp_id in to_save:\n",
    "                try:\n",
    "                    try:\n",
    "                        num_maj = to_save[[snp_id]].value_counts()[1]\n",
    "                    except Exception as e:\n",
    "                        num_maj = 0\n",
    "                    try:\n",
    "                        num_het = to_save[[snp_id]].value_counts()[0]\n",
    "                    except Exception as e:\n",
    "                        num_het = 0\n",
    "                    try:\n",
    "                        num_min = to_save[[snp_id]].value_counts()[-1]\n",
    "                    except Exception as e:\n",
    "                        num_min = 0\n",
    "                    total_humans = num_maj + num_het + num_min\n",
    "                    maf = (num_min*2 + num_het)/(total_humans*2)\n",
    "        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"snp {snp_id} has a problem\")\n",
    "        \n",
    "                tot_mafs.append(maf)\n",
    "        \n",
    "            max_maf = np.round(max(tot_mafs), 5)\n",
    "            min_maf = np.round(min(tot_mafs), 5)\n",
    "            size = to_save.shape[1]\n",
    "        \n",
    "            # Save the processed cluster data\n",
    "            to_save.to_pickle(f\"{PATH_output}/{num}_{size}_maf_{min_maf}_{max_maf}.pkl\")\n",
    "    \n",
    "        path_one_hot_genotype = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks_one_hot/{pop}\"\n",
    "        os.system(f\"rm -rf {path_one_hot_genotype}\")\n",
    "        os.makedirs(path_one_hot_genotype, exist_ok = True)\n",
    "    \n",
    "        path_lds = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks/{pop}\"\n",
    "        ld_files = os.listdir(path_lds)\n",
    "        for ld_file in ld_files:\n",
    "            path_ld_file = path_lds + \"/\" + ld_file\n",
    "            ld_complete = pd.read_pickle(path_ld_file)\n",
    "        \n",
    "            # Create db_minor\n",
    "            db_minor = ld_complete.copy()\n",
    "            db_minor = db_minor.applymap(lambda x: 1 if x == -1.0 else 0)\n",
    "            \n",
    "            # Create db_het\n",
    "            db_het = ld_complete.copy()\n",
    "            db_het = db_het.applymap(lambda x: 1 if x == 0.0 else 0)\n",
    "            \n",
    "            # Create db_major\n",
    "            db_major = ld_complete.copy()\n",
    "            db_major = db_major.applymap(lambda x: 1 if x == 1.0 else 0)\n",
    "            \n",
    "        \n",
    "            db_minor.to_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_minor.pkl\")\n",
    "            db_het.to_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_het.pkl\")\n",
    "            db_major.to_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_major.pkl\")\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
