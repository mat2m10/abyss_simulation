{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e3528-f435-4c07-92e9-b74c5f062df9",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6528f9ad-fbc6-4822-8965-d5e70ce3156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0 = all messages are logged (default), 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import time  # Import the time module\n",
    "import warnings\n",
    "import importlib.util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats import t, entropy, stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, Input, Model, layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "from helpers import (\n",
    "    parse_variables, get_risk_level, hi_gauss_blob_risk_fun, blob_risk_fun, \n",
    "    NW_risk_fun, square_risk_fun, map_to_color, simulate_quant_trait\n",
    ")\n",
    "\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487575fc-b220-49b6-9ef8-d1b027620a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "if 'k' not in globals():\n",
    "    k = int(dict['k'])\n",
    "    \n",
    "if 'M' not in globals():\n",
    "    M = float(dict['M'])\n",
    "    \n",
    "HWE = int(dict['HWE'])\n",
    "\n",
    "nr_humans = int(dict['nr_humans'])\n",
    "nr_snps = int(dict['nr_snps'])\n",
    "bottleneck_nr = int(dict['bottleneck_nr'])\n",
    "\n",
    "# Thresholds\n",
    "very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "\n",
    "rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "\n",
    "common_threshold_L = float(dict['common_threshold_L'])\n",
    "common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80e4e39-c78b-4e06-8948-1665faef4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/01_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd20320-65c5-4ba5-9770-8d6f9442e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = very_rare.rename(columns=lambda x: 'VR' + x)/2\n",
    "rare = rare.rename(columns=lambda x: 'R' + x)/2\n",
    "common = common.rename(columns=lambda x: 'C' + x)/2\n",
    "complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "complete = ((complete*2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00341f68-3941-44a6-9255-f72b53af0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bottle = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/phenotype/abyss_bottleneck\"\n",
    "bottle_file = [f for f in os.listdir(path_bottle) if int(f.split(\"_\")[2]) ==  bottleneck_nr][0]\n",
    "elapsed_time_bottleneck = float(bottle_file.split('_')[3].split('seconds')[0])\n",
    "bottle = pd.read_pickle(f\"{path_bottle}/{bottle_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb470fa-be9d-49d5-b0cd-facb535506a5",
   "metadata": {},
   "source": [
    "# Run Abyss on LD block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6dc24c5-995c-48d5-859d-b35e04382eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maf_prediction(bottle_in, geno_out, epoch, patience):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(bottle_in, geno_out, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Regularization parameter\n",
    "    l2_regularizer = 0.001\n",
    "    \n",
    "    # Original autoencoder model with L2 regularization\n",
    "    decoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(int(nr_snps/2), activation='elu', input_shape=(bottle_in.shape[1],), kernel_regularizer=regularizers.l2(l2_regularizer)),  # First hidden layer with L2 regularization\n",
    "        layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('elu'),\n",
    "        tf.keras.layers.Dense(geno_out.shape[1], activation='linear', kernel_regularizer=regularizers.l2(l2_regularizer))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Compile the original model with L2 regularization\n",
    "    decoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['mean_absolute_error'])\n",
    "    \n",
    "    # Define Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Fit the original model with Early Stopping\n",
    "    history = decoder.fit(X_train, y_train, epochs=epoch, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    return decoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91492263-9f4d-4c05-9c6e-aa35ce0304ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pop in bottle['cluster'].unique():\n",
    "    temp_bottle = bottle[bottle['cluster'] == pop]\n",
    "    temp_bottle = temp_bottle.drop('cluster', axis=1)\n",
    "    path_output = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks_estimated_mafs/{pop}\"\n",
    "    os.system(f\"rm -rf {path_output}\")\n",
    "    os.makedirs(path_output, exist_ok = True)\n",
    "    path_one_hot_genotype = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks_one_hot/{pop}\"\n",
    "    path_lds = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks/{pop}\"\n",
    "    ld_files = os.listdir(path_lds)\n",
    "    epoch = 500\n",
    "    patience = 50\n",
    "    p2s = []\n",
    "    twopqs = []\n",
    "    q2s = []\n",
    "    for ld_file in ld_files:\n",
    "        db_minor = pd.read_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_minor.pkl\")\n",
    "        db_het = pd.read_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_het.pkl\")\n",
    "        db_major = pd.read_pickle(f\"{path_one_hot_genotype}/{ld_file.split('.pkl')[0]}_db_major.pkl\")\n",
    "        \n",
    "        start_time_p2 = time.time()\n",
    "        decoder, history = maf_prediction(temp_bottle, db_major, epoch, patience)\n",
    "        end_time_p2 = time.time()\n",
    "        \n",
    "        elapsed_time_p2 = np.round(end_time_p2 - start_time_p2,3)\n",
    "        p2 = decoder(tf.convert_to_tensor(temp_bottle, dtype=tf.float32))\n",
    "        p2 = pd.DataFrame(data=p2, columns = db_major.columns)\n",
    "    \n",
    "        p2.index = db_major.index\n",
    "        p2s.append(p2)\n",
    "        \n",
    "        start_time_2pq = time.time()\n",
    "        decoder, history = maf_prediction(temp_bottle, db_het, epoch, patience)\n",
    "        end_time_2pq = time.time()\n",
    "        \n",
    "        elapsed_time_2pq = np.round(end_time_2pq - start_time_2pq,3)\n",
    "        \n",
    "        twopq = decoder(tf.convert_to_tensor(temp_bottle, dtype=tf.float32))\n",
    "        twopq = pd.DataFrame(data=twopq, columns = db_het.columns)  \n",
    "        twopq.index = db_het.index\n",
    "        twopqs.append(twopq)\n",
    "        \n",
    "        start_time_q2 = time.time()\n",
    "        decoder, history = maf_prediction(temp_bottle, db_minor, epoch, patience)\n",
    "        end_time_q2 = time.time()\n",
    "        \n",
    "        elapsed_time_q2 = np.round(end_time_q2 - start_time_q2,3)\n",
    "        q2 = decoder(tf.convert_to_tensor(temp_bottle, dtype=tf.float32))\n",
    "        q2 = pd.DataFrame(data=q2, columns = db_minor.columns)\n",
    "        \n",
    "        q2.index = db_minor.index\n",
    "        q2s.append(q2)\n",
    "        \n",
    "        path_output_global = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}_HWE{HWE}/genotype/LD_blocks_estimated_mafs/{pop}\"\n",
    "        p2.to_pickle(f\"{path_output_global}/{ld_file}_esti_p2_via_esti_pop_{elapsed_time_p2}seconds.pkl\")\n",
    "        twopq.to_pickle(f\"{path_output_global}/{ld_file}_esti_2pq_via_esti_pop_{elapsed_time_2pq}seconds.pkl\")        \n",
    "        q2.to_pickle(f\"{path_output_global}/{ld_file}_esti_q2_via_esti_pop_{elapsed_time_q2}seconds.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1ef5cc-3417-4578-8e4f-823624c60869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\np2 = pd.concat(p2s, axis=1)\\np2 = p2.sort_index()\\np2 = p2[list(complete.columns)]\\n\\nq2 = pd.concat(q2s, axis=1)\\nq2 = q2.sort_index()\\nq2 = q2[list(complete.columns)]\\n\\ntwopq = pd.concat(twopqs, axis=1)\\ntwopq = twopq.sort_index()\\ntwopq = twopq[list(complete.columns)]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "p2 = pd.concat(p2s, axis=1)\n",
    "p2 = p2.sort_index()\n",
    "p2 = p2[list(complete.columns)]\n",
    "\n",
    "q2 = pd.concat(q2s, axis=1)\n",
    "q2 = q2.sort_index()\n",
    "q2 = q2[list(complete.columns)]\n",
    "\n",
    "twopq = pd.concat(twopqs, axis=1)\n",
    "twopq = twopq.sort_index()\n",
    "twopq = twopq[list(complete.columns)]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
