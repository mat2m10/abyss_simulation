{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f14c941-5e69-45a2-bdc7-46e09e1d1ded",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f771fa6-8444-40a1-91d0-ef572f9886e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 13:02:13.610848: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-23 13:02:13.797762: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-23 13:02:13.981610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-23 13:02:14.099459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-23 13:02:14.100268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-23 13:02:14.320825: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-23 13:02:15.906192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from helpers import parse_variables, get_risk_level, hi_gauss_blob_risk_fun, blob_risk_fun, NW_risk_fun, square_risk_fun, map_to_color\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import importlib.util\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "from helpers import parse_variables, get_risk_level, map_to_color, simulate_quant_trait\n",
    "from models import ols_regression, manhattan_linear, gc\n",
    "from deep_learning_models import abyss, deep_abyss\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy import stats\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11516e9c-9a16-4267-b872-359d52a16e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = parse_variables('geno_simulation.txt')\n",
    "G = int(dict['G'])\n",
    "L = int(dict['L'])\n",
    "c = int(dict['c'])\n",
    "k = int(dict['k'])\n",
    "M = float(dict['M'])\n",
    "\n",
    "# Thresholds\n",
    "very_rare_threshold_L = float(dict['very_rare_threshold_L'])\n",
    "very_rare_threshold_H = float(dict['very_rare_threshold_H'])\n",
    "\n",
    "rare_threshold_L = float(dict['rare_threshold_L'])\n",
    "rare_threshold_H = float(dict['rare_threshold_H'])\n",
    "\n",
    "common_threshold_L = float(dict['common_threshold_L'])\n",
    "common_threshold_H = float(dict['common_threshold_H'])\n",
    "\n",
    "number_of_snps = (G*L)/2 # one loci per chromosome\n",
    "number_of_individuals = c*k*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9b5b34-cbfa-4446-9ce7-ce67aed3abb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/G40_L100_c50_k4_M0.99/genotype/02_veryrare_genotype_AF_0.0_0.05.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m very_rare \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/G\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mG\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_L\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_c\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_k\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_M\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mM\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/genotype/02_veryrare_genotype_AF_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvery_rare_threshold_L\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvery_rare_threshold_H\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m rare \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/02_rare_genotype_AF_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrare_threshold_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrare_threshold_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m common \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_M\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/genotype/02_common_genotype_AF_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_threshold_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_threshold_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/abyss_simul/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/abyss_simul/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/G40_L100_c50_k4_M0.99/genotype/02_veryrare_genotype_AF_0.0_0.05.pkl'"
     ]
    }
   ],
   "source": [
    "very_rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_veryrare_genotype_AF_{very_rare_threshold_L}_{very_rare_threshold_H}.pkl\")\n",
    "rare = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_rare_genotype_AF_{rare_threshold_L}_{rare_threshold_H}.pkl\")\n",
    "common = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/genotype/02_common_genotype_AF_{common_threshold_L}_{common_threshold_H}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfcb4ec-50a3-4a2c-88a7-f457766e0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_rare = very_rare.rename(columns=lambda x: 'VR' + x)/2\n",
    "rare = rare.rename(columns=lambda x: 'R' + x)/2\n",
    "common = common.rename(columns=lambda x: 'C' + x)/2\n",
    "complete = pd.concat([common, rare, very_rare], axis=1)\n",
    "complete = ((complete*2)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d625bb-e0ae-4636-a956-e0fa24abea4e",
   "metadata": {},
   "source": [
    "# Load populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878ab77-22f4-42f9-9c58-b7f62131c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/01_population_structure.pkl\")\n",
    "populations['population_number'] = populations['populations'].str.extract('(\\d+)').astype(int)\n",
    "# Calculating X and Y coordinates\n",
    "populations['x'] = ((populations['population_number'] - 1) % k) + 1\n",
    "populations['y'] = ((populations['population_number'] - 1) // k) + 1\n",
    "populations['z'] = 0.5\n",
    "populations['population'] = populations['population_number']/(k*k)\n",
    "palette = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "\n",
    "# Standardize the vector\n",
    "std_dev = np.std(populations['x'])\n",
    "mean = np.mean(populations['x'])\n",
    "populations['x'] = np.round((populations['x'] - mean) / std_dev,2)\n",
    "\n",
    "std_dev = np.std(populations['y'])\n",
    "mean = np.mean(populations['y'])\n",
    "populations['y'] = np.round((populations['y'] - mean) / std_dev,2)\n",
    "\n",
    "# Check the grid\n",
    "df_agg = populations.groupby(['x', 'y']).agg({'population': 'mean'}).reset_index()\n",
    "\n",
    "# Now, pivot the aggregated DataFrame\n",
    "grid_df = df_agg.pivot(index='y', columns='x', values='population')\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(grid_df, cmap=palette, linewidths=.5, square=True, cbar=False)\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title('Population Grid', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Sometimes it's necessary to invert the y-axis for correct orientation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4690d-59f8-44a8-875d-a4cd3fa773cd",
   "metadata": {},
   "source": [
    "# Run abyss on everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a7690-2584-4788-a61e-b22de3c528f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abyss(geno_in, geno_out, bottleneck_nr, epoch, patience):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(geno_in, geno_out, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Regularization parameter\n",
    "    l2_regularizer = 0.001\n",
    "    \n",
    "    # Original autoencoder model with L2 regularization\n",
    "    autoencoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100, activation='elu', input_shape=(geno_in.shape[1],), kernel_regularizer=regularizers.l2(l2_regularizer)),  # First hidden layer with L2 regularization\n",
    "        tf.keras.layers.Dense(bottleneck_nr, activation='elu', name='bottleneck', kernel_regularizer=regularizers.l2(l2_regularizer)),  # Bottleneck layer with L2 regularization\n",
    "        layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('elu'),\n",
    "        tf.keras.layers.Dense(100, activation='elu', kernel_regularizer=regularizers.l2(l2_regularizer)),  # Second hidden layer with L2 regularization\n",
    "        tf.keras.layers.Dense(geno_out.shape[1], activation='linear', kernel_regularizer=regularizers.l2(l2_regularizer))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Compile the original model with L2 regularization\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['mean_absolute_error'])\n",
    "    \n",
    "    # Define Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Fit the original model with Early Stopping\n",
    "    history = autoencoder.fit(X_train, y_train, epochs=epoch, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Extract the bottleneck layer after fitting the model\n",
    "    bottleneck_model = tf.keras.Model(inputs=autoencoder.inputs, outputs=autoencoder.get_layer('bottleneck').output)\n",
    "    \n",
    "    return autoencoder, bottleneck_model, history\n",
    "\n",
    "# Function to ensure minimum cluster size\n",
    "def ensure_min_cluster_size(data, labels, min_size):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    small_clusters = unique_labels[counts < min_size]\n",
    "    \n",
    "    # Reassign points from small clusters to the nearest large cluster\n",
    "    for cluster in small_clusters:\n",
    "        indices = np.where(labels == cluster)[0]\n",
    "        for index in indices:\n",
    "            # Find nearest large cluster\n",
    "            nearest_large_cluster = None\n",
    "            nearest_distance = float('inf')\n",
    "            for label in unique_labels:\n",
    "                if label not in small_clusters:\n",
    "                    distance = np.linalg.norm(data.iloc[index] - kmeans.cluster_centers_[label])\n",
    "                    if distance < nearest_distance:\n",
    "                        nearest_distance = distance\n",
    "                        nearest_large_cluster = label\n",
    "            labels[index] = nearest_large_cluster\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e50128-e8e6-413a-950a-5c4cba114493",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f1273-fdc2-438f-ad0a-8b0fcb992e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_nr = 2 # for viz\n",
    "epoch = 300\n",
    "patience = 10\n",
    "dim_labels = [f\"dim{i}\" for i in range(1, bottleneck_nr + 1)]\n",
    "autoencoder, bottleneck_model, history = abyss(complete, complete, bottleneck_nr, epoch, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a1422-e7f4-45cf-b1ba-8df5de474386",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_2dim = pd.DataFrame(data=bottleneck_model(complete), columns = dim_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15452833-25b6-40b6-a1d9-23221b99b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "populations = pd.read_pickle(f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/01_population_structure.pkl\")\n",
    "colors = [map_to_color(x, y, z, populations) for x, y, z in zip(populations['x'], populations['y'], populations['z'])]\n",
    "sns.scatterplot(x='dim1', y='dim2', data=bottle_2dim, color=colors, linewidth=0)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.title(f\"Autoencoder bottleneck\")\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d33f3-73d7-4135-85aa-588ac0d6b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_nr = 64 # for clustering\n",
    "epoch = 300\n",
    "patience = 10\n",
    "dim_labels = [f\"dim{i}\" for i in range(1, bottleneck_nr + 1)]\n",
    "autoencoder, bottleneck_model, history = abyss(complete, complete, bottleneck_nr, epoch, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cb512-d256-4e8b-a983-8d0b53cb566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle = pd.DataFrame(data=bottleneck_model(complete), columns = dim_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126cab7-9b7d-4f67-845a-408549b0de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = c\n",
    "num_clus = int(np.round(len(bottle)/sample_size))\n",
    "size_min = sample_size - round(sample_size / 5)\n",
    "size_max = sample_size + round(sample_size / 5)\n",
    "\n",
    "# Apply constrained K-Means clustering\n",
    "clf = KMeansConstrained(\n",
    "    n_clusters=num_clus,\n",
    "    size_min=size_min,\n",
    "    size_max=size_max,\n",
    "    random_state=0\n",
    ")\n",
    "clf.fit_predict(np.array(bottle))\n",
    "bottle['cluster'] = clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ede1d-0100-4caa-85cc-aa59ee163e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_2dim['cluster'] = bottle['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a568eb-db1e-474b-83bf-f55955315e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='dim1', y='dim2', data=bottle_2dim, hue='cluster', linewidth=0)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.title(f\"Autoencoder bottleneck\")\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796bf8b-f287-4b77-9928-0bbd3cc3d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bottle = f\"data/G{G}_L{L}_c{c}_k{k}_M{M}/phenotype/abyss_bottleneck\"\n",
    "os.makedirs(path_bottle, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666c467-2b30-496b-bf1f-703ef1cb6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle.to_pickle(f\"{path_bottle}/abyss_bottleneck_{bottleneck_nr}_dims_0_zoom.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbfd38-09e9-42cc-83dc-7fc4381206e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
